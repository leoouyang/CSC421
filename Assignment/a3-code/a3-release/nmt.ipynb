{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"nmt.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python2","display_name":"Python 2"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"TjPTaRB4mpCd","colab_type":"text"},"cell_type":"markdown","source":["# Colab FAQ\n","\n","For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n","\n","You need to use the colab GPU for this assignmentby selecting:\n","\n","> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"]},{"metadata":{"id":"s9IS9B9-yUU5","colab_type":"text"},"cell_type":"markdown","source":["## Setup PyTorch\n","All files are stored at /content/csc421/a3/ folder\n"]},{"metadata":{"id":"Z-6MQhMOlHXD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":659},"outputId":"a88adff6-c55b-4247-8631-ee4065c62e26","executionInfo":{"status":"ok","timestamp":1552954751748,"user_tz":240,"elapsed":10962,"user":{"displayName":"Leo Ouyang","photoUrl":"","userId":"13328508759078244531"}}},"cell_type":"code","source":["######################################################################\n","# Setup python environment and change the current working directory\n","######################################################################\n","!pip install torch torchvision\n","!pip install Pillow==4.0.0\n","%mkdir -p /content/csc421/a3/\n","%cd /content/csc421/a3"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python2.7/dist-packages (1.0.1.post2)\n","Requirement already satisfied: torchvision in /usr/local/lib/python2.7/dist-packages (0.2.2.post3)\n","Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from torchvision) (1.11.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from torchvision) (1.14.6)\n","Collecting pillow>=4.1.1 (from torchvision)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/f3/421598450cb9503f4565d936860763b5af413a61009d87a5ab1e34139672/Pillow-5.4.1-cp27-cp27mu-manylinux1_x86_64.whl (2.0MB)\n","\u001b[K    100% |████████████████████████████████| 2.0MB 11.0MB/s \n","\u001b[31mfastai 0.7.0 has requirement torch<0.4, but you'll have torch 1.0.1.post2 which is incompatible.\u001b[0m\n","\u001b[31mimgaug 0.2.8 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n","\u001b[?25hInstalling collected packages: pillow\n","  Found existing installation: Pillow 4.0.0\n","    Uninstalling Pillow-4.0.0:\n","      Successfully uninstalled Pillow-4.0.0\n","Successfully installed pillow-5.4.1\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL"]}}},"metadata":{"tags":[]}},{"output_type":"stream","text":["Collecting Pillow==4.0.0\n","  Using cached https://files.pythonhosted.org/packages/89/99/0e3522a9764fe371bf9f7729404b1ef7d9c4fc49cbe5f1761c6e07812345/Pillow-4.0.0-cp27-cp27mu-manylinux1_x86_64.whl\n","Requirement already satisfied: olefile in /usr/local/lib/python2.7/dist-packages (from Pillow==4.0.0) (0.46)\n","\u001b[31mfastai 0.7.0 has requirement torch<0.4, but you'll have torch 1.0.1.post2 which is incompatible.\u001b[0m\n","\u001b[31mimgaug 0.2.8 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n","\u001b[31mtorchvision 0.2.2.post3 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n","Installing collected packages: Pillow\n","  Found existing installation: Pillow 5.4.1\n","    Uninstalling Pillow-5.4.1:\n","      Successfully uninstalled Pillow-5.4.1\n","Successfully installed Pillow-4.0.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL"]}}},"metadata":{"tags":[]}},{"output_type":"stream","text":["/content/csc421/a3\n"],"name":"stdout"}]},{"metadata":{"id":"9DaTdRNuUra7","colab_type":"text"},"cell_type":"markdown","source":["# Helper code"]},{"metadata":{"id":"4BIpGwANoQOg","colab_type":"text"},"cell_type":"markdown","source":["## Utility functions"]},{"metadata":{"id":"D-UJHBYZkh7f","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","import pdb\n","import argparse\n","import pickle as pkl\n","\n","from collections import defaultdict\n","\n","import numpy as np\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","from six.moves.urllib.request import urlretrieve\n","import tarfile\n","import pickle\n","import sys\n","\n","\n","def get_file(fname,\n","             origin,\n","             untar=False,\n","             extract=False,\n","             archive_format='auto',\n","             cache_dir='data'):\n","    datadir = os.path.join(cache_dir)\n","    if not os.path.exists(datadir):\n","        os.makedirs(datadir)\n","\n","    if untar:\n","        untar_fpath = os.path.join(datadir, fname)\n","        fpath = untar_fpath + '.tar.gz'\n","    else:\n","        fpath = os.path.join(datadir, fname)\n","    \n","    print(fpath)\n","    if not os.path.exists(fpath):\n","        print('Downloading data from', origin)\n","\n","        error_msg = 'URL fetch failure on {}: {} -- {}'\n","        try:\n","            try:\n","                urlretrieve(origin, fpath)\n","            except URLError as e:\n","                raise Exception(error_msg.format(origin, e.errno, e.reason))\n","            except HTTPError as e:\n","                raise Exception(error_msg.format(origin, e.code, e.msg))\n","        except (Exception, KeyboardInterrupt) as e:\n","            if os.path.exists(fpath):\n","                os.remove(fpath)\n","            raise\n","\n","    if untar:\n","        if not os.path.exists(untar_fpath):\n","            print('Extracting file.')\n","            with tarfile.open(fpath) as archive:\n","                archive.extractall(datadir)\n","        return untar_fpath\n","\n","    if extract:\n","        _extract_archive(fpath, datadir, archive_format)\n","\n","    return fpath\n","\n","class AttrDict(dict):\n","    def __init__(self, *args, **kwargs):\n","        super(AttrDict, self).__init__(*args, **kwargs)\n","        self.__dict__ = self\n","        \n","def to_var(tensor, cuda):\n","    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n","\n","        Arguments:\n","            tensor: A Tensor object.\n","            cuda: A boolean flag indicating whether to use the GPU.\n","\n","        Returns:\n","            A Variable object, on the GPU if cuda==True.\n","    \"\"\"\n","    if cuda:\n","        return Variable(tensor.cuda())\n","    else:\n","        return Variable(tensor)\n","\n","\n","def create_dir_if_not_exists(directory):\n","    \"\"\"Creates a directory if it doesn't already exist.\n","    \"\"\"\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n","\n","\n","def save_loss_plot(train_losses, val_losses, opts):\n","    \"\"\"Saves a plot of the training and validation loss curves.\n","    \"\"\"\n","    plt.figure()\n","    plt.plot(range(len(train_losses)), train_losses)\n","    plt.plot(range(len(val_losses)), val_losses)\n","    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n","    plt.xlabel('Epochs', fontsize=16)\n","    plt.ylabel('Loss', fontsize=16)\n","    plt.xticks(fontsize=14)\n","    plt.yticks(fontsize=14)\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n","    plt.close()\n","\n","\n","def checkpoint(encoder, decoder, idx_dict, opts):\n","    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n","    contains the char_to_index and index_to_char mappings, and the start_token\n","    and end_token values.\n","    \"\"\"\n","    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n","        torch.save(encoder, f)\n","\n","    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n","        torch.save(decoder, f)\n","\n","    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n","        pkl.dump(idx_dict, f)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pbvpn4MaV0I1","colab_type":"text"},"cell_type":"markdown","source":["## Data loader"]},{"metadata":{"id":"XVT4TNTOV3Eg","colab_type":"code","colab":{}},"cell_type":"code","source":["def read_lines(filename):\n","    \"\"\"Read a file and split it into lines.\n","    \"\"\"\n","    lines = open(filename).read().strip().lower().split('\\n')\n","    return lines\n","\n","\n","def read_pairs(filename):\n","    \"\"\"Reads lines that consist of two words, separated by a space.\n","\n","    Returns:\n","        source_words: A list of the first word in each line of the file.\n","        target_words: A list of the second word in each line of the file.\n","    \"\"\"\n","    lines = read_lines(filename)\n","    source_words, target_words = [], []\n","    for line in lines:\n","        line = line.strip()\n","        if line:\n","            source, target = line.split()\n","            source_words.append(source)\n","            target_words.append(target)\n","    return source_words, target_words\n","\n","\n","def all_alpha_or_dash(s):\n","    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\n","    \"\"\"\n","    return all(c.isalpha() or c == '-' for c in s)\n","\n","\n","def filter_lines(lines):\n","    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\n","    \"\"\"\n","    return [line for line in lines if all_alpha_or_dash(line)]\n","\n","\n","def load_data():\n","    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\n","    \"\"\"\n","\n","    source_lines, target_lines = read_pairs('data/pig_latin_data.txt')\n","\n","    # Filter lines\n","    source_lines = filter_lines(source_lines)\n","    target_lines = filter_lines(target_lines)\n","\n","    all_characters = set(''.join(source_lines)) | set(''.join(target_lines))\n","\n","    # Create a dictionary mapping each character to a unique index\n","    char_to_index = { char: index for (index, char) in enumerate(sorted(list(all_characters))) }\n","\n","    # Add start and end tokens to the dictionary\n","    start_token = len(char_to_index)\n","    end_token = len(char_to_index) + 1\n","    char_to_index['SOS'] = start_token\n","    char_to_index['EOS'] = end_token\n","\n","    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n","    index_to_char = { index: char for (char, index) in char_to_index.items() }\n","\n","    # Store the final size of the vocabulary\n","    vocab_size = len(char_to_index)\n","\n","    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n","\n","    idx_dict = { 'char_to_index': char_to_index,\n","                 'index_to_char': index_to_char,\n","                 'start_token': start_token,\n","                 'end_token': end_token }\n","\n","    return line_pairs, vocab_size, idx_dict\n","\n","\n","def create_dict(pairs):\n","    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n","    This is used to make batches: each batch consists of two parallel tensors, one containing\n","    all source indexes and the other containing all corresponding target indexes.\n","    Within a batch, all the source words are the same length, and all the target words are\n","    the same length.\n","    \"\"\"\n","    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n","\n","    d = defaultdict(list)\n","    for (s,t) in unique_pairs:\n","        d[(len(s), len(t))].append((s,t))\n","\n","    return d\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bRWfRdmVVjUl","colab_type":"text"},"cell_type":"markdown","source":["## Training and evaluation code"]},{"metadata":{"id":"wa5-onJhoSeM","colab_type":"code","colab":{}},"cell_type":"code","source":["def string_to_index_list(s, char_to_index, end_token):\n","    \"\"\"Converts a sentence into a list of indexes (for each character).\n","    \"\"\"\n","    return [char_to_index[char] for char in s] + [end_token]  # Adds the end token to each index list\n","\n","\n","def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n","    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n","    words (whitespace-separated), running the encoder-decoder model to translate each\n","    word independently, and then stitching the words back together with spaces between them.\n","    \"\"\"\n","    if idx_dict is None:\n","      line_pairs, vocab_size, idx_dict = load_data()\n","    return ' '.join([translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()])\n","\n","\n","def translate(input_string, encoder, decoder, idx_dict, opts):\n","    \"\"\"Translates a given string from English to Pig-Latin.\n","    \"\"\"\n","\n","    char_to_index = idx_dict['char_to_index']\n","    index_to_char = idx_dict['index_to_char']\n","    start_token = idx_dict['start_token']\n","    end_token = idx_dict['end_token']\n","\n","    max_generated_chars = 20\n","    gen_string = ''\n","\n","    indexes = string_to_index_list(input_string, char_to_index, end_token)\n","    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n","\n","    encoder_annotations, encoder_last_hidden = encoder(indexes)\n","\n","    decoder_hidden = encoder_last_hidden\n","    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n","    decoder_inputs = decoder_input\n","\n","    for i in range(max_generated_chars):\n","      ## slow decoding, recompute everything at each time\n","      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n","      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n","      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n","      ni = ni[-1] #latest output token\n","\n","      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n","      \n","      if ni == end_token:\n","          break\n","      else:\n","          gen_string = \"\".join(\n","              [index_to_char[int(item)] \n","               for item in generated_words.cpu().numpy().reshape(-1)])\n","\n","    return gen_string\n","\n","\n","def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n","    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\n","    \"\"\"\n","    if idx_dict is None:\n","      line_pairs, vocab_size, idx_dict = load_data()\n","    char_to_index = idx_dict['char_to_index']\n","    index_to_char = idx_dict['index_to_char']\n","    start_token = idx_dict['start_token']\n","    end_token = idx_dict['end_token']\n","\n","    max_generated_chars = 20\n","    gen_string = ''\n","\n","    indexes = string_to_index_list(input_string, char_to_index, end_token)\n","    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n","\n","    encoder_annotations, encoder_hidden = encoder(indexes)\n","\n","    decoder_hidden = encoder_hidden\n","    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n","    decoder_inputs = decoder_input\n","\n","    produced_end_token = False\n","\n","    for i in range(max_generated_chars):\n","      ## slow decoding, recompute everything at each time\n","      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n","      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n","      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n","      ni = ni[-1] #latest output token\n","      \n","      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n","      \n","      if ni == end_token:\n","          break\n","      else:\n","          gen_string = \"\".join(\n","              [index_to_char[int(item)] \n","               for item in generated_words.cpu().numpy().reshape(-1)])\n","    \n","    if isinstance(attention_weights, tuple):\n","      ## transformer's attention mweights\n","      attention_weights, self_attention_weights = attention_weights\n","    \n","    all_attention_weights = attention_weights.data.cpu().numpy()\n","    \n","    for i in range(len(all_attention_weights)):\n","      attention_weights_matrix = all_attention_weights[i].squeeze()\n","      fig = plt.figure()\n","      ax = fig.add_subplot(111)\n","      cax = ax.matshow(attention_weights_matrix, cmap='bone')\n","      fig.colorbar(cax)\n","\n","      # Set up axes\n","      ax.set_yticklabels([''] + list(input_string) + ['EOS'], rotation=90)\n","      ax.set_xticklabels([''] + list(gen_string) + (['EOS'] if produced_end_token else []))\n","\n","      # Show label at every tick\n","      ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","      ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","      # Add title\n","      plt.xlabel('Attention weights to the source sentence in layer {}'.format(i+1))\n","      plt.tight_layout()\n","      plt.grid('off')\n","      plt.show()\n","      #plt.savefig(save)\n","\n","      #plt.close(fig)\n","\n","    return gen_string\n","\n","\n","def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n","    \"\"\"Train/Evaluate the model on a dataset.\n","\n","    Arguments:\n","        data_dict: The validation/test word pairs, organized by source and target lengths.\n","        encoder: An encoder model to produce annotations for each step of the input sequence.\n","        decoder: A decoder model (with or without attention) to generate output tokens.\n","        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n","        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n","        optimizer: Train the weights if an optimizer is given. None if only evaluate the model. \n","        opts: The command-line arguments.\n","\n","    Returns:\n","        mean_loss: The average loss over all batches from data_dict.\n","    \"\"\"\n","    start_token = idx_dict['start_token']\n","    end_token = idx_dict['end_token']\n","    char_to_index = idx_dict['char_to_index']\n","\n","    losses = []\n","    for key in data_dict:\n","        input_strings, target_strings = zip(*data_dict[key])\n","        input_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in input_strings]\n","        target_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in target_strings]\n","\n","        num_tensors = len(input_tensors)\n","        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n","\n","        for i in range(num_batches):\n","\n","            start = i * opts.batch_size\n","            end = start + opts.batch_size\n","\n","            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n","            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n","\n","            # The batch size may be different in each epoch\n","            BS = inputs.size(0)\n","\n","            encoder_annotations, encoder_hidden = encoder(inputs)\n","\n","            # The last hidden state of the encoder becomes the first hidden state of the decoder\n","            decoder_hidden = encoder_hidden\n","\n","            start_vector = torch.ones(BS).long().unsqueeze(1) * start_token  # BS x 1 --> 16x1  CHECKED\n","            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n","\n","            loss = 0.0\n","\n","            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n","\n","            decoder_inputs = torch.cat([decoder_input, targets[:, 0:-1]], dim=1)  # Gets decoder inputs by shifting the targets to the right \n","            \n","            decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, encoder_hidden)\n","            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n","            targets_flatten = targets.view(-1)\n","            loss = criterion(decoder_outputs_flatten, targets_flatten)\n","\n","            losses.append(loss.item())\n","\n","            ## training if an optimizer is provided\n","            if optimizer:\n","              # Zero gradients\n","              optimizer.zero_grad()\n","              # Compute gradients\n","              loss.backward()\n","              # Update the parameters of the encoder and decoder\n","              optimizer.step()\n","              \n","    mean_loss = np.mean(losses)\n","    return mean_loss\n","\n","  \n","\n","def training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts):\n","    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n","        * Prints training and val loss each epoch.\n","        * Prints qualitative translation results each epoch using TEST_SENTENCE\n","        * Saves an attention map for TEST_WORD_ATTN each epoch\n","\n","    Arguments:\n","        train_dict: The training word pairs, organized by source and target lengths.\n","        val_dict: The validation word pairs, organized by source and target lengths.\n","        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n","        encoder: An encoder model to produce annotations for each step of the input sequence.\n","        decoder: A decoder model (with or without attention) to generate output tokens.\n","        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n","        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n","        opts: The command-line arguments.\n","    \"\"\"\n","\n","    start_token = idx_dict['start_token']\n","    end_token = idx_dict['end_token']\n","    char_to_index = idx_dict['char_to_index']\n","\n","    loss_log = open(os.path.join(opts.checkpoint_path, 'loss_log.txt'), 'w')\n","\n","    best_val_loss = 1e6\n","    train_losses = []\n","    val_losses = []\n","\n","    for epoch in range(opts.nepochs):\n","\n","        optimizer.param_groups[0]['lr'] *= opts.lr_decay\n","        \n","        train_loss = compute_loss(train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\n","        val_loss = compute_loss(val_dict, encoder, decoder, idx_dict, criterion, None, opts)\n","\n","        if val_loss < best_val_loss:\n","            checkpoint(encoder, decoder, idx_dict, opts)\n","\n","        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n","        print(\"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(epoch, train_loss, val_loss, gen_string))\n","\n","        loss_log.write('{} {} {}\\n'.format(epoch, train_loss, val_loss))\n","        loss_log.flush()\n","\n","        train_losses.append(train_loss)\n","        val_losses.append(val_loss)\n","\n","        save_loss_plot(train_losses, val_losses, opts)\n","\n","\n","def print_data_stats(line_pairs, vocab_size, idx_dict):\n","    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\n","    \"\"\"\n","    print('=' * 80)\n","    print('Data Stats'.center(80))\n","    print('-' * 80)\n","    for pair in line_pairs[:5]:\n","        print(pair)\n","    print('Num unique word pairs: {}'.format(len(line_pairs)))\n","    print('Vocabulary: {}'.format(idx_dict['char_to_index'].keys()))\n","    print('Vocab size: {}'.format(vocab_size))\n","    print('=' * 80)\n","\n","\n","def train(opts):\n","    line_pairs, vocab_size, idx_dict = load_data()\n","    print_data_stats(line_pairs, vocab_size, idx_dict)\n","\n","    # Split the line pairs into an 80% train and 20% val split\n","    num_lines = len(line_pairs)\n","    num_train = int(0.8 * num_lines)\n","    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n","\n","    # Group the data by the lengths of the source and target words, to form batches\n","    train_dict = create_dict(train_pairs)\n","    val_dict = create_dict(val_pairs)\n","\n","    ##########################################################################\n","    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n","    ##########################################################################\n","    encoder = GRUEncoder(vocab_size=vocab_size, \n","                         hidden_size=opts.hidden_size, \n","                         opts=opts)\n","\n","    if opts.decoder_type == 'rnn':\n","        decoder = RNNDecoder(vocab_size=vocab_size, \n","                             hidden_size=opts.hidden_size)\n","    elif opts.decoder_type == 'rnn_attention':\n","        decoder = RNNAttentionDecoder(vocab_size=vocab_size, \n","                                      hidden_size=opts.hidden_size, \n","                                      attention_type=opts.attention_type)\n","    elif opts.decoder_type == 'transformer':\n","        decoder = TransformerDecoder(vocab_size=vocab_size, \n","                                     hidden_size=opts.hidden_size, \n","                                     num_layers=opts.num_transformer_layers)\n","    else:\n","        raise NotImplementedError\n","        \n","    #### setup checkpoint path\n","    model_name = 'h{}-bs{}-{}'.format(opts.hidden_size, \n","                                      opts.batch_size, \n","                                      opts.decoder_type)\n","    opts.checkpoint_path = model_name\n","    create_dir_if_not_exists(opts.checkpoint_path)\n","    ####\n","\n","    if opts.cuda:\n","        encoder.cuda()\n","        decoder.cuda()\n","        print(\"Moved models to GPU!\")\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate)\n","\n","    try:\n","        training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts)\n","    except KeyboardInterrupt:\n","        print('Exiting early from training.')\n","        return encoder, decoder\n","      \n","    return encoder, decoder\n","\n","\n","def print_opts(opts):\n","    \"\"\"Prints the values of all command-line arguments.\n","    \"\"\"\n","    print('=' * 80)\n","    print('Opts'.center(80))\n","    print('-' * 80)\n","    for key in opts.__dict__:\n","        print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n","    print('=' * 80)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bXNsLNkOn38w","colab_type":"text"},"cell_type":"markdown","source":["# Your code for NMT models"]},{"metadata":{"id":"_BAfi_8yWB3y","colab_type":"text"},"cell_type":"markdown","source":["## GRU cell"]},{"metadata":{"id":"9ztmyA5Ro67o","colab_type":"code","colab":{}},"cell_type":"code","source":["class MyGRUCell(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(MyGRUCell, self).__init__()\n","\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","\n","        # ------------\n","        # FILL THIS IN\n","        # ------------\n","        # Input linear layers\n","        self.Wiz = nn.Linear(input_size, hidden_size, bias = False)\n","        self.Wir = nn.Linear(input_size, hidden_size, bias = False)\n","        self.Win = nn.Linear(input_size, hidden_size, bias = False)\n","\n","        # Hidden linear layers\n","        self.Whz = nn.Linear(hidden_size, hidden_size)\n","        self.Whr = nn.Linear(hidden_size, hidden_size)\n","        self.Whn = nn.Linear(hidden_size, hidden_size)\n","        \n","\n","\n","    def forward(self, x, h_prev):\n","        \"\"\"Forward pass of the GRU computation for one time step.\n","\n","        Arguments\n","            x: batch_size x input_size\n","            h_prev: batch_size x hidden_size\n","\n","        Returns:\n","            h_new: batch_size x hidden_size\n","        \"\"\"\n","\n","        # ------------\n","        # FILL THIS IN\n","        # ------------\n","        z = F.sigmoid(self.Wiz(x) + self.Whz(h_prev))\n","        r = F.sigmoid(self.Wir(x) + self.Whr(h_prev))\n","        g = F.tanh(self.Win(x) + r * self.Whn(h_prev))\n","        h_new = (1-z)*g + z*h_prev\n","        return h_new\n","\n","        "],"execution_count":0,"outputs":[]},{"metadata":{"id":"-JBVFLEZWNC1","colab_type":"text"},"cell_type":"markdown","source":["### GRU encoder / decoder"]},{"metadata":{"id":"xaDt7XDmWRzC","colab_type":"code","colab":{}},"cell_type":"code","source":["class GRUEncoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, opts):\n","        super(GRUEncoder, self).__init__()\n","\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","        self.opts = opts\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","        self.gru = nn.GRUCell(hidden_size, hidden_size)\n","\n","    def forward(self, inputs):\n","        \"\"\"Forward pass of the encoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n","\n","        Returns:\n","            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n","        \"\"\"\n","\n","        batch_size, seq_len = inputs.size()\n","        hidden = self.init_hidden(batch_size)\n","\n","        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n","        annotations = []\n","\n","        for i in range(seq_len):\n","            x = encoded[:,i,:]  # Get the current time step, across the whole batch\n","            hidden = self.gru(x, hidden)\n","            annotations.append(hidden)\n","\n","        annotations = torch.stack(annotations, dim=1)\n","        return annotations, hidden\n","\n","    def init_hidden(self, bs):\n","        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n","        of a batch of sequences.\n","\n","        Arguments:\n","            bs: The batch size for the initial hidden state.\n","\n","        Returns:\n","            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n","        \"\"\"\n","        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)\n","\n","\n","class RNNDecoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size):\n","        super(RNNDecoder, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","        self.rnn = nn.GRUCell(input_size=hidden_size, hidden_size=hidden_size)\n","        self.out = nn.Linear(hidden_size, vocab_size)\n","\n","    def forward(self, inputs, annotations, hidden_init):\n","        \"\"\"Forward pass of the non-attentional decoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch. (batch_size x seq_len)\n","            annotations: This is not used here. It just maintains consistency with the\n","                    interface used by the AttentionDecoder class.\n","            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n","\n","        Returns:\n","            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n","            None\n","        \"\"\"        \n","        batch_size, seq_len = inputs.size()\n","        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n","\n","        hiddens = []\n","        h_prev = hidden_init\n","        for i in range(seq_len):\n","            x = embed[:,i,:]  # Get the current time step input tokens, across the whole batch\n","            h_prev = self.rnn(x, h_prev)  # batch_size x hidden_size\n","            hiddens.append(h_prev)\n","\n","        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n","        \n","        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n","        return output, None      \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tWe0RO5FWajD","colab_type":"text"},"cell_type":"markdown","source":["## Attention"]},{"metadata":{"id":"9GUK5A7CWhV8","colab_type":"code","colab":{}},"cell_type":"code","source":["class AdditiveAttention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(AdditiveAttention, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","\n","        # A two layer fully-connected network\n","        # hidden_size*2 --> hidden_size, ReLU, hidden_size --> 1\n","        self.attention_network = nn.Sequential(\n","                                    nn.Linear(hidden_size*2, hidden_size),\n","                                    nn.ReLU(),\n","                                    nn.Linear(hidden_size, 1)\n","                                 )\n","\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, queries, keys, values):\n","        \"\"\"The forward pass of the additive attention mechanism.\n","\n","        Arguments:\n","            queries: The current decoder hidden state. (batch_size x hidden_size)\n","            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","\n","        Returns:\n","            context: weighted average of the values (batch_size x 1 x hidden_size)\n","            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n","\n","            The attention_weights must be a softmax weighting over the seq_len annotations.\n","        \"\"\"\n","\n","        # ------------\n","        # FILL THIS IN\n","        # ------------\n","        batch_size, seq_len, hidden_size = keys.size()\n","        expanded_queries = queries.unsqueeze(1).expand_as(keys) #size is batch_size x seq_len x hidden_size\n","        concat_inputs = torch.cat((expanded_queries, keys),2)\n","        unnormalized_attention = self.attention_network(concat_inputs.view(-1, hidden_size * 2)).view(batch_size, seq_len, 1)\n","        attention_weights = self.softmax(unnormalized_attention) #size is batch_size x seq_len x 1\n","        context = torch.bmm(attention_weights.transpose(1,2), values)\n","        return context, attention_weights\n","        \n","      \n","\n","class ScaledDotAttention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(ScaledDotAttention, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","\n","        self.Q = nn.Linear(hidden_size, hidden_size)\n","        self.K = nn.Linear(hidden_size, hidden_size)\n","        self.V = nn.Linear(hidden_size, hidden_size)\n","        self.softmax = nn.Softmax(dim=1)\n","        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n","\n","    def forward(self, queries, keys, values):\n","        \"\"\"The forward pass of the scaled dot attention mechanism.\n","\n","        Arguments:\n","            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n","            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","\n","        Returns:\n","            context: weighted average of the values (batch_size x k x hidden_size)\n","            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n","\n","            The output must be a softmax weighting over the seq_len annotations.\n","        \"\"\"\n","\n","        # ------------\n","        # FILL THIS IN\n","        # ------------\n","        batch_size, seq_len, hidden_size = keys.size()\n","        q = self.Q(queries.view(-1, hidden_size)).view(batch_size, -1, hidden_size) #batch_size x (k) x hidden_size\n","        k = self.K(keys.view(-1,hidden_size)).view(batch_size, seq_len, hidden_size) #batch_size x seq_len x hidden_size\n","        v = self.V(values.view(-1, hidden_size)).view(batch_size, seq_len, hidden_size) #batch_size x seq_len x hidden_size\n","        unnormalized_attention = self.scaling_factor * torch.bmm(k, q.transpose(1,2)) #batch_size x seq_len x k\n","        attention_weights = self.softmax(unnormalized_attention) #batch_size x seq_len x k\n","        context = torch.bmm(attention_weights.transpose(1,2), v) #batch_size x k x hidden_size\n","        return context, attention_weights\n","        \n","\n","      \n","      \n","class CausalScaledDotAttention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(CausalScaledDotAttention, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","        self.neg_inf = torch.tensor(-1e7)\n","\n","        self.Q = nn.Linear(hidden_size, hidden_size)\n","        self.K = nn.Linear(hidden_size, hidden_size)\n","        self.V = nn.Linear(hidden_size, hidden_size)\n","        self.softmax = nn.Softmax(dim=1)\n","        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n","\n","    def forward(self, queries, keys, values):\n","        \"\"\"The forward pass of the scaled dot attention mechanism.\n","\n","        Arguments:\n","            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n","            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","\n","        Returns:\n","            context: weighted average of the values (batch_size x k x hidden_size)\n","            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n","\n","            The output must be a softmax weighting over the seq_len annotations.\n","        \"\"\"\n","\n","        # ------------\n","        # FILL THIS IN\n","        # ------------\n","        batch_size, seq_len, hidden_size = keys.size()\n","        q = self.Q(queries.view(-1, hidden_size)).view(batch_size, -1, hidden_size) #batch_size x (k) x hidden_size\n","        k = self.K(keys.view(-1,hidden_size)).view(batch_size, seq_len, hidden_size) #batch_size x seq_len x hidden_size\n","        v = self.V(values.view(-1, hidden_size)).view(batch_size, seq_len, hidden_size) #batch_size x seq_len x hidden_size\n","        unnormalized_attention = self.scaling_factor * torch.bmm(k, q.transpose(1,2)) #batch_size x seq_len x k\n","        # I think this function is only used in self-attention, where q,k,v have same dimention. k = seq_len\n","        mask = torch.tril(torch.ones(batch_size,seq_len,seq_len,dtype=torch.uint8)).transpose(1,2)\n","        unnormalized_attention[mask==0] = self.neg_inf\n","        attention_weights = self.softmax(unnormalized_attention)\n","        context = torch.bmm(attention_weights.transpose(1,2), v)\n","        return context, attention_weights\n","        \n","        "],"execution_count":0,"outputs":[]},{"metadata":{"id":"pemjZo2XWtRt","colab_type":"text"},"cell_type":"markdown","source":["### Attention decoder"]},{"metadata":{"id":"PfjF0Z-PWwPv","colab_type":"code","colab":{}},"cell_type":"code","source":["class RNNAttentionDecoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, attention_type='scaled_dot'):\n","        super(RNNAttentionDecoder, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","\n","        self.rnn = MyGRUCell(input_size=hidden_size*2, hidden_size=hidden_size)\n","        if attention_type == 'additive':\n","          self.attention = AdditiveAttention(hidden_size=hidden_size)\n","        elif attention_type == 'scaled_dot':\n","          self.attention = ScaledDotAttention(hidden_size=hidden_size)\n","        \n","        self.out = nn.Linear(hidden_size, vocab_size)\n","\n","        \n","    def forward(self, inputs, annotations, hidden_init):\n","        \"\"\"Forward pass of the attention-based decoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n","            annotations: The encoder hidden states for each step of the input.\n","                         sequence. (batch_size x seq_len x hidden_size)\n","            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n","\n","        Returns:\n","            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n","            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n","        \"\"\"\n","        \n","        batch_size, seq_len = inputs.size()\n","        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n","\n","        hiddens = []\n","        attentions = []\n","        h_prev = hidden_init\n","        for i in range(seq_len):\n","            # ------------\n","            # FILL THIS IN\n","            # ------------\n","            embed_current = embed[:,i,:]\n","            context, attention_weights = self.attention(embed_current, annotations, annotations)\n","            embed_and_context = torch.cat((embed_current, context.squeeze(1)), 1)\n","            h_prev = self.rnn(embed_and_context,h_prev)\n","\n","            \n","            hiddens.append(h_prev)\n","            attentions.append(attention_weights)\n","\n","        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n","        attentions = torch.cat(attentions, dim=2) # batch_size x seq_len x seq_len\n","        \n","        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n","        return output, attentions\n","        "],"execution_count":0,"outputs":[]},{"metadata":{"id":"N8JpcwTRW5cw","colab_type":"text"},"cell_type":"markdown","source":["### Transformer decoder"]},{"metadata":{"id":"V5vJPku1W7sz","colab_type":"code","colab":{}},"cell_type":"code","source":["class TransformerDecoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, num_layers):\n","        super(TransformerDecoder, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)        \n","        self.num_layers = num_layers\n","        \n","        self.self_attentions = nn.ModuleList([CausalScaledDotAttention(\n","                                    hidden_size=hidden_size, \n","                                 ) for i in range(self.num_layers)])\n","        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n","                                    hidden_size=hidden_size, \n","                                 ) for i in range(self.num_layers)])\n","        self.attention_mlps = nn.ModuleList([nn.Sequential(\n","                                    nn.Linear(hidden_size, hidden_size),\n","                                    nn.ReLU(),\n","                                 ) for i in range(self.num_layers)])\n","        self.out = nn.Linear(hidden_size, vocab_size)\n","\n","        \n","    def forward(self, inputs, annotations, hidden_init):\n","        \"\"\"Forward pass of the attention-based decoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n","            annotations: The encoder hidden states for each step of the input.\n","                         sequence. (batch_size x seq_len x hidden_size)\n","            hidden_init: Not used in the transformer decoder\n","        Returns:\n","            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n","            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n","        \"\"\"\n","        \n","        batch_size, seq_len = inputs.size()\n","        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n","\n","        encoder_attention_weights_list = []\n","        self_attention_weights_list = []\n","        contexts = embed\n","        for i in range(self.num_layers):\n","          # ------------\n","          # FILL THIS IN\n","          # ------------\n","          new_contexts, self_attention_weights = self.self_attentions[i](contexts, contexts, contexts)\n","          residual_contexts = contexts + new_contexts\n","          new_contexts, encoder_attention_weights = self.encoder_attentions[i](residual_contexts,annotations, annotations)\n","          residual_contexts = residual_contexts + new_contexts\n","          new_contexts = self.attention_mlps[i](residual_contexts.view(-1, self.hidden_size)).view(batch_size, seq_len, self.hidden_size)\n","          contexts = residual_contexts + new_contexts\n","\n","          \n","          encoder_attention_weights_list.append(encoder_attention_weights)\n","          self_attention_weights_list.append(self_attention_weights)\n","          \n","        output = self.out(contexts)\n","        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n","        self_attention_weights = torch.stack(self_attention_weights_list)\n","        \n","        return output, (encoder_attention_weights, self_attention_weights)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XuNFd6LNo0-o","colab_type":"text"},"cell_type":"markdown","source":["# Training\n"]},{"metadata":{"id":"kiUwiOITHTW4","colab_type":"text"},"cell_type":"markdown","source":["## Download dataset"]},{"metadata":{"id":"xwcFjsEpHRbI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"75e90b7f-34e4-44a3-b181-743a61b1fe09","executionInfo":{"status":"ok","timestamp":1552954753440,"user_tz":240,"elapsed":12512,"user":{"displayName":"Leo Ouyang","photoUrl":"","userId":"13328508759078244531"}}},"cell_type":"code","source":["######################################################################\n","# Download Translation datasets\n","######################################################################\n","data_fpath = get_file(fname='pig_latin_data.txt', \n","                         origin='http://www.cs.toronto.edu/~jba/pig_latin_data.txt', \n","                         untar=False)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["data/pig_latin_data.txt\n","('Downloading data from', 'http://www.cs.toronto.edu/~jba/pig_latin_data.txt')\n"],"name":"stdout"}]},{"metadata":{"id":"hmQmyJDSRFKR","colab_type":"text"},"cell_type":"markdown","source":["## RNN decoder"]},{"metadata":{"id":"0LKaRF1jwhH7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":2249},"outputId":"0eb5da19-4f0f-4301-a5cf-9052517995b0","executionInfo":{"status":"ok","timestamp":1552956867631,"user_tz":240,"elapsed":248349,"user":{"displayName":"Leo Ouyang","photoUrl":"","userId":"13328508759078244531"}}},"cell_type":"code","source":["TEST_SENTENCE = 'the air conditioning is working'\n","#ethay airway onditioningcay isway orkingway\n","\n","torch.manual_seed(1)\n","args = AttrDict()\n","args_dict = {\n","              'cuda':True, \n","              'nepochs':100, \n","              'checkpoint_dir':\"checkpoints\", \n","              'learning_rate':0.005, \n","              'lr_decay':0.99,\n","              'batch_size':64, \n","              'hidden_size':20, \n","              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n","              'attention_type': '',  # options: additive / scaled_dot\n","}\n","args.update(args_dict)\n","\n","print_opts(args)\n","rnn_encoder, rnn_decoder = train(args)\n","\n","translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":38,"outputs":[{"output_type":"stream","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                            hidden_size: 20                                     \n","                          learning_rate: 0.005                                  \n","                             batch_size: 64                                     \n","                                nepochs: 100                                    \n","                                   cuda: 1                                      \n","                         checkpoint_dir: checkpoints                            \n","                           decoder_type: rnn                                    \n","                               lr_decay: 0.99                                   \n","                         attention_type:                                        \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('payment', 'aymentpay')\n","('ordination', 'ordinationway')\n","('amends', 'amendsway')\n","('principally', 'incipallypray')\n","('anybody', 'anybodyway')\n","Num unique word pairs: 6387\n","Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 2.401 | Val loss: 2.026 | Gen: eeray ay eereray ensay eereray\n","Epoch:   1 | Train loss: 1.946 | Val loss: 1.851 | Gen: eray array ontessay ingay onterray\n","Epoch:   2 | Train loss: 1.785 | Val loss: 1.749 | Gen: eray array ontingsay ingway ongay\n","Epoch:   3 | Train loss: 1.669 | Val loss: 1.673 | Gen: eray arreday onssssay ingway ontay-otay-ingway\n","Epoch:   4 | Train loss: 1.587 | Val loss: 1.629 | Gen: eray aingray ontingsay indway outhay-ingway\n","Epoch:   5 | Train loss: 1.516 | Val loss: 1.567 | Gen: eday airthay ontingsay ingway ortiongay\n","Epoch:   6 | Train loss: 1.454 | Val loss: 1.520 | Gen: emay airedway ontingstay indway outhay-ayy\n","Epoch:   7 | Train loss: 1.402 | Val loss: 1.484 | Gen: emay airedway ontingstay ingway ouththay\n","Epoch:   8 | Train loss: 1.366 | Val loss: 1.472 | Gen: eeday aithay onsingstay inway outhtway\n","Epoch:   9 | Train loss: 1.327 | Val loss: 1.429 | Gen: emay airway onkingsway iway outionday\n","Epoch:  10 | Train loss: 1.295 | Val loss: 1.413 | Gen: emay airway onsingstay iway outionday\n","Epoch:  11 | Train loss: 1.256 | Val loss: 1.387 | Gen: emay airway onsingstay iway outionday\n","Epoch:  12 | Train loss: 1.229 | Val loss: 1.361 | Gen: emay airway onsingstay iway outhtybay\n","Epoch:  13 | Train loss: 1.204 | Val loss: 1.353 | Gen: eeway airway onsingstay iway outiondray\n","Epoch:  14 | Train loss: 1.175 | Val loss: 1.316 | Gen: ehedway airay onsingstay iway outiongray\n","Epoch:  15 | Train loss: 1.149 | Val loss: 1.314 | Gen: eeway aithay onsingstay iway outiontray\n","Epoch:  16 | Train loss: 1.128 | Val loss: 1.276 | Gen: ehedway aithay onsingstybay isway outionray\n","Epoch:  17 | Train loss: 1.112 | Val loss: 1.287 | Gen: ehedway ayithay onkingstay iway outthtway\n","Epoch:  18 | Train loss: 1.098 | Val loss: 1.262 | Gen: ehedway aithay onsingstybay isway outionray\n","Epoch:  19 | Train loss: 1.089 | Val loss: 1.289 | Gen: eheday ayithay onsingionday iway outingray\n","Epoch:  20 | Train loss: 1.092 | Val loss: 1.212 | Gen: eheway ay-iorday onsingstyway isway outithtway\n","Epoch:  21 | Train loss: 1.047 | Val loss: 1.202 | Gen: eheway ay-iorday onsingionday isway outithtway\n","Epoch:  22 | Train loss: 1.023 | Val loss: 1.190 | Gen: eheday ayitray onkingshay isway outithtway\n","Epoch:  23 | Train loss: 0.999 | Val loss: 1.168 | Gen: ehedway ayithay ontingsionway isway outithtway\n","Epoch:  24 | Train loss: 0.994 | Val loss: 1.163 | Gen: ehedway ayithay onstinghtway isway outithtway\n","Epoch:  25 | Train loss: 0.986 | Val loss: 1.223 | Gen: eheday ayithay onstiondsay isway outionray\n","Epoch:  26 | Train loss: 0.981 | Val loss: 1.155 | Gen: ehedway ayitray onostiongsay isway outithtway\n","Epoch:  27 | Train loss: 0.984 | Val loss: 1.190 | Gen: eheway aithay onstingstay isway outhtedway\n","Epoch:  28 | Train loss: 0.953 | Val loss: 1.133 | Gen: eheday ayitray onostiongay isway outithtway\n","Epoch:  29 | Train loss: 0.934 | Val loss: 1.119 | Gen: eheday aithay ontingsionway isway outithtway\n","Epoch:  30 | Train loss: 0.927 | Val loss: 1.124 | Gen: eheway ayitray onostiongay isway outithtway\n","Epoch:  31 | Train loss: 0.915 | Val loss: 1.121 | Gen: eheday aithay ontingsionway isway outithtway\n","Epoch:  32 | Train loss: 0.915 | Val loss: 1.131 | Gen: eheday ayitray onstinghingway isway outitypay\n","Epoch:  33 | Train loss: 0.910 | Val loss: 1.136 | Gen: eheday aithay onkingstway isway outithtway\n","Epoch:  34 | Train loss: 0.892 | Val loss: 1.111 | Gen: eheday aithay onousingway isway outithtway\n","Epoch:  35 | Train loss: 0.897 | Val loss: 1.120 | Gen: eheday aithay onosingiontay isway outinghay\n","Epoch:  36 | Train loss: 0.918 | Val loss: 1.112 | Gen: eheway ayithay ontingshay isway outhingway\n","Epoch:  37 | Train loss: 0.903 | Val loss: 1.116 | Gen: ehedway airtay onstinghtnay isway outitypay\n","Epoch:  38 | Train loss: 0.885 | Val loss: 1.177 | Gen: eheday aithay onstingionway isway outingpay\n","Epoch:  39 | Train loss: 0.880 | Val loss: 1.102 | Gen: eheday aitray onkingstway isway outiontray\n","Epoch:  40 | Train loss: 0.850 | Val loss: 1.085 | Gen: eheday aittay onsingiontay isway outingpay\n","Epoch:  41 | Train loss: 0.845 | Val loss: 1.081 | Gen: ehay airway oncionsionay isway outingpay\n","Epoch:  42 | Train loss: 0.846 | Val loss: 1.070 | Gen: eheway airway onsingiontay isway outingpray\n","Epoch:  43 | Train loss: 0.833 | Val loss: 1.099 | Gen: ehay airway oncionsingway isway outiontay\n","Epoch:  44 | Train loss: 0.838 | Val loss: 1.064 | Gen: ehay airway onkingstway isway outingpay\n","Epoch:  45 | Train loss: 0.956 | Val loss: 1.133 | Gen: eheway airupay ontingsthay isay outitioncay\n","Epoch:  46 | Train loss: 0.881 | Val loss: 1.090 | Gen: eheway aitray oncionstionday issay outithay\n","Epoch:  47 | Train loss: 0.837 | Val loss: 1.058 | Gen: ehay airray oncionstionday isway outingpray\n","Epoch:  48 | Train loss: 0.809 | Val loss: 1.033 | Gen: ehay airway oncionsiontray isway outingpay\n","Epoch:  49 | Train loss: 0.799 | Val loss: 1.036 | Gen: ehay airway onstiontionway isway outtionway\n","Epoch:  50 | Train loss: 0.797 | Val loss: 1.089 | Gen: ehay airway oncionstionday isway outtionway\n","Epoch:  51 | Train loss: 0.825 | Val loss: 1.084 | Gen: ehay airday onciostionday isway outinghay\n","Epoch:  52 | Train loss: 0.804 | Val loss: 1.068 | Gen: ehay airupay onsingiontay isway outtionway\n","Epoch:  53 | Train loss: 0.793 | Val loss: 1.057 | Gen: ehay airway onsingiontray isway outtionway\n","Epoch:  54 | Train loss: 0.783 | Val loss: 1.036 | Gen: ehay airupay oncionsionray isway outtionway\n","Epoch:  55 | Train loss: 0.778 | Val loss: 1.066 | Gen: ehay airway oncionstiontay isway outhingway\n","Epoch:  56 | Train loss: 0.789 | Val loss: 1.049 | Gen: ehay airway oncionstionday isway outtionway\n","Epoch:  57 | Train loss: 0.780 | Val loss: 1.016 | Gen: ehay airupay onsinghtnay isway outtionway\n","Epoch:  58 | Train loss: 0.777 | Val loss: 1.010 | Gen: eheway airway oncionsiontray isway outtionway\n","Epoch:  59 | Train loss: 0.784 | Val loss: 1.064 | Gen: ehay airway oncionsiontay isway ouctiontay\n","Epoch:  60 | Train loss: 0.797 | Val loss: 1.063 | Gen: ehay airway oncionsiontray isway outinghay\n","Epoch:  61 | Train loss: 0.783 | Val loss: 1.007 | Gen: ehay airupay oncionstiontay isway outingpay\n","Epoch:  62 | Train loss: 0.761 | Val loss: 0.994 | Gen: ehay airupay oncionstiontay isway ouctiontray\n","Epoch:  63 | Train loss: 0.741 | Val loss: 0.994 | Gen: ehay airupay onciostioncay isway ouctiontray\n","Epoch:  64 | Train loss: 0.753 | Val loss: 1.023 | Gen: ehay airupay oncionstionday isway outinghay\n","Epoch:  65 | Train loss: 0.766 | Val loss: 1.074 | Gen: ehay airway oncionsiontray isway outcybay\n","Epoch:  66 | Train loss: 0.763 | Val loss: 0.995 | Gen: ehay airupay oncionsthay isway ouctiontay\n","Epoch:  67 | Train loss: 0.738 | Val loss: 0.991 | Gen: ehay airupay onciostingway isway ouctiontay\n","Epoch:  68 | Train loss: 0.726 | Val loss: 1.007 | Gen: ehay airway oncionsiontray isway ouctingray\n","Epoch:  69 | Train loss: 0.726 | Val loss: 1.018 | Gen: ehay airway oncionstpay isway ouctiontay\n","Epoch:  70 | Train loss: 0.746 | Val loss: 0.991 | Gen: ehay airway oncionsiontray isway ouctiontay\n","Epoch:  71 | Train loss: 0.742 | Val loss: 0.970 | Gen: ehay airupay onciostingway isway ouctiontay\n","Epoch:  72 | Train loss: 0.724 | Val loss: 0.968 | Gen: ehay airupay oncionstionmay isway ouctingray\n","Epoch:  73 | Train loss: 0.725 | Val loss: 0.990 | Gen: ehay airupay oncionstionmay isway ouctionmay\n","Epoch:  74 | Train loss: 0.740 | Val loss: 0.975 | Gen: ehay airway oncionstionmay isway ouctingray\n","Epoch:  75 | Train loss: 0.711 | Val loss: 0.956 | Gen: ehay airway oncionstionmay isway ouctingray\n","Epoch:  76 | Train loss: 0.724 | Val loss: 0.999 | Gen: ehay airway oncionstionmay isway ouctingray\n","Epoch:  77 | Train loss: 0.704 | Val loss: 0.968 | Gen: ehay airway oncionstionmay isway ouctingray\n","Epoch:  78 | Train loss: 0.708 | Val loss: 0.951 | Gen: ehay airupay oncistionday isway ouctingray\n","Epoch:  79 | Train loss: 0.704 | Val loss: 1.012 | Gen: ehtay airway oncionsiontray isway ouctingray\n","Epoch:  80 | Train loss: 0.708 | Val loss: 0.973 | Gen: ehay airupay oncionstionmay isway ouctingray\n","Epoch:  81 | Train loss: 0.707 | Val loss: 0.965 | Gen: ehtay airupay oncionstionmay isway ouctingway\n","Epoch:  82 | Train loss: 0.701 | Val loss: 0.938 | Gen: ehay airupay onciostingcay isway ouctingray\n","Epoch:  83 | Train loss: 0.692 | Val loss: 0.949 | Gen: ehay airupay oncionsiontray isway ouctingray\n","Epoch:  84 | Train loss: 0.683 | Val loss: 0.963 | Gen: ehay airway oncistionsway isway ouctingway\n","Epoch:  85 | Train loss: 0.689 | Val loss: 0.992 | Gen: ehtay airway oncionsiontray isway ouctingray\n","Epoch:  86 | Train loss: 0.708 | Val loss: 0.950 | Gen: ehay airway oncistionday isway ouctingray\n","Epoch:  87 | Train loss: 0.698 | Val loss: 0.940 | Gen: ehtay airway oncistionday isway ouctingway\n","Epoch:  88 | Train loss: 0.788 | Val loss: 0.975 | Gen: ehtay airupay oncionsiontay isway ouctingray\n","Epoch:  89 | Train loss: 0.737 | Val loss: 0.943 | Gen: ehay airway oncistioncay isway ouctingway\n","Epoch:  90 | Train loss: 0.680 | Val loss: 0.920 | Gen: ehay airway oncistioncay isway ouctingray\n","Epoch:  91 | Train loss: 0.670 | Val loss: 0.923 | Gen: ehay airway oncistingcay isway ouctingray\n","Epoch:  92 | Train loss: 0.660 | Val loss: 0.915 | Gen: ehay airway oncistionstay isway ouctingray\n","Epoch:  93 | Train loss: 0.656 | Val loss: 0.921 | Gen: ehay airway oncistingcay isway ouctingray\n","Epoch:  94 | Train loss: 0.653 | Val loss: 0.925 | Gen: ehay airway oncistingcay isway ouctingray\n","Epoch:  95 | Train loss: 0.652 | Val loss: 0.929 | Gen: ehay airway oncistingcay isway ouctingray\n","Epoch:  96 | Train loss: 0.653 | Val loss: 0.919 | Gen: ehtay airway oncistingcay isway ouctingway\n","Epoch:  97 | Train loss: 0.658 | Val loss: 0.954 | Gen: ehay airway oncistingcay isway ouctingray\n","Epoch:  98 | Train loss: 0.659 | Val loss: 0.916 | Gen: ehay airway oncistioncray isway ouctingray\n","Epoch:  99 | Train loss: 0.661 | Val loss: 0.941 | Gen: ehtay airway onitingsshay isway ouctingray\n","source:\t\tthe air conditioning is working \n","translated:\tehtay airway onitingsshay isway ouctingray\n"],"name":"stdout"}]},{"metadata":{"id":"p2kPGj5DFv7a","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"f4e47370-46a3-4388-ab7a-733ad0b6db58","executionInfo":{"status":"ok","timestamp":1552956896075,"user_tz":240,"elapsed":316,"user":{"displayName":"Leo Ouyang","photoUrl":"","userId":"13328508759078244531"}}},"cell_type":"code","source":["TEST_SENTENCE = 'but you are encouraged to submit a proposal earlier'\n","translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":39,"outputs":[{"output_type":"stream","text":["source:\t\tbut you are encouraged to submit a proposal earlier \n","translated:\tuthay youway areway encouraysay otay uitpedway away oppasclyday earlerway\n"],"name":"stdout"}]},{"metadata":{"id":"7cP7nl5NRJbu","colab_type":"text"},"cell_type":"markdown","source":["## RNN attention decoder"]},{"metadata":{"id":"nKlyfbuPDXDR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":2249},"outputId":"0eef92e8-5b6d-4c98-f0a9-ad869dfad851","executionInfo":{"status":"ok","timestamp":1552959645026,"user_tz":240,"elapsed":700927,"user":{"displayName":"Leo Ouyang","photoUrl":"","userId":"13328508759078244531"}}},"cell_type":"code","source":["TEST_SENTENCE = 'the air conditioning is working'\n","\n","torch.manual_seed(1)\n","args = AttrDict()\n","args_dict = {\n","              'cuda':True, \n","              'nepochs':100, \n","              'checkpoint_dir':\"checkpoints\", \n","              'learning_rate':0.005, \n","              'lr_decay':0.99,\n","              'batch_size':64, \n","              'hidden_size':20, \n","              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n","              'attention_type': 'additive',  # options: additive / scaled_dot\n","}\n","args.update(args_dict)\n","\n","print_opts(args)\n","rnn_attn_encoder, rnn_attn_decoder = train(args)\n","\n","translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":49,"outputs":[{"output_type":"stream","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                            hidden_size: 20                                     \n","                          learning_rate: 0.005                                  \n","                             batch_size: 64                                     \n","                                nepochs: 100                                    \n","                                   cuda: 1                                      \n","                         checkpoint_dir: checkpoints                            \n","                           decoder_type: rnn_attention                          \n","                               lr_decay: 0.99                                   \n","                         attention_type: additive                               \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('payment', 'aymentpay')\n","('ordination', 'ordinationway')\n","('amends', 'amendsway')\n","('principally', 'incipallypray')\n","('anybody', 'anybodyway')\n","Num unique word pairs: 6387\n","Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 2.362 | Val loss: 1.943 | Gen: eray allay ongsay ongsay ongray\n","Epoch:   1 | Train loss: 1.822 | Val loss: 1.680 | Gen: eay arttay ongstingsay onssay ongstay\n","Epoch:   2 | Train loss: 1.573 | Val loss: 1.465 | Gen: eay airtay ongstay ississay onghonghay\n","Epoch:   3 | Train loss: 1.356 | Val loss: 1.262 | Gen: eay airirililililililili ongstay issisisisisisisisisi ongstay\n","Epoch:   4 | Train loss: 1.126 | Val loss: 1.064 | Gen: eay aiririririririririri ongscay isssay orkingway\n","Epoch:   5 | Train loss: 0.915 | Val loss: 0.907 | Gen: eureway aiririririririririri ondutindcay isisisisway orkingway\n","Epoch:   6 | Train loss: 0.772 | Val loss: 0.793 | Gen: ehay airirway onlingstay issway orkingway\n","Epoch:   7 | Train loss: 0.667 | Val loss: 0.730 | Gen: ehay airway ondiondkay issway orkingway\n","Epoch:   8 | Train loss: 0.607 | Val loss: 0.685 | Gen: ehay airway ondiondindcay issway orkingway\n","Epoch:   9 | Train loss: 0.516 | Val loss: 0.607 | Gen: ehay airway ondiondindkay issway orkingway\n","Epoch:  10 | Train loss: 0.489 | Val loss: 0.561 | Gen: ehay airway onlindiondindway isway orkingway\n","Epoch:  11 | Train loss: 0.463 | Val loss: 0.521 | Gen: ehay airway onlingcingcay issway orkingway\n","Epoch:  12 | Train loss: 0.416 | Val loss: 0.508 | Gen: ehay airway ondiongcay issway orkingway\n","Epoch:  13 | Train loss: 0.378 | Val loss: 0.469 | Gen: ehay airway oniningcay issway orkingway\n","Epoch:  14 | Train loss: 0.348 | Val loss: 0.458 | Gen: ehay airway onlingcay isway orkingway\n","Epoch:  15 | Train loss: 0.337 | Val loss: 0.448 | Gen: ehay airway onlitiongcay issway orkingway\n","Epoch:  16 | Train loss: 0.331 | Val loss: 0.454 | Gen: ehay airway oningcay issway orkingway\n","Epoch:  17 | Train loss: 0.312 | Val loss: 0.427 | Gen: ehay airway onlongcay isway orkingway\n","Epoch:  18 | Train loss: 0.282 | Val loss: 0.404 | Gen: ehay airway onlongcay isway orkingway\n","Epoch:  19 | Train loss: 0.255 | Val loss: 0.395 | Gen: ehay airway oningcay-ongcay isway orkingway\n","Epoch:  20 | Train loss: 0.255 | Val loss: 0.436 | Gen: ehay airway ondiondkay isway orkingway\n","Epoch:  21 | Train loss: 0.263 | Val loss: 0.387 | Gen: ehaypthay airway onlongcay isway orkingway\n","Epoch:  22 | Train loss: 0.243 | Val loss: 0.353 | Gen: ethay airway onloningcay isway orkingway\n","Epoch:  23 | Train loss: 0.215 | Val loss: 0.364 | Gen: ehay airway oningcay isway orkingway\n","Epoch:  24 | Train loss: 0.204 | Val loss: 0.343 | Gen: ehay-oethay airway oningcay-ongcay isway orkingway\n","Epoch:  25 | Train loss: 0.198 | Val loss: 0.363 | Gen: ethay airway onloningcay isway orkingway\n","Epoch:  26 | Train loss: 0.191 | Val loss: 0.328 | Gen: ethay airway oningcay-ongcay isway orkingway\n","Epoch:  27 | Train loss: 0.208 | Val loss: 0.423 | Gen: ehay airway ondioningcay isway orkingway\n","Epoch:  28 | Train loss: 0.234 | Val loss: 0.339 | Gen: ethay airway onloningcay isway orkingway\n","Epoch:  29 | Train loss: 0.191 | Val loss: 0.334 | Gen: ethay airway onloningcay isway orkingway\n","Epoch:  30 | Train loss: 0.172 | Val loss: 0.317 | Gen: ethay airway onloningcay isway orkingway\n","Epoch:  31 | Train loss: 0.159 | Val loss: 0.305 | Gen: ethay airway ongtiongcay isway orkingway\n","Epoch:  32 | Train loss: 0.151 | Val loss: 0.304 | Gen: ethay airway ongcay-ongcay isway orkingway\n","Epoch:  33 | Train loss: 0.146 | Val loss: 0.304 | Gen: ethay airway oningcay-ongcay isway orkingway\n","Epoch:  34 | Train loss: 0.142 | Val loss: 0.308 | Gen: ethay airway oningcay-ongcay isway orkingway\n","Epoch:  35 | Train loss: 0.139 | Val loss: 0.302 | Gen: ethay airway oningcay-ongcay isway orkingway\n","Epoch:  36 | Train loss: 0.136 | Val loss: 0.308 | Gen: ethay airway oningcay-ongcay isway orkingway\n","Epoch:  37 | Train loss: 0.152 | Val loss: 0.328 | Gen: ethay airway ongcay-oningcay isway orkingway\n","Epoch:  38 | Train loss: 0.182 | Val loss: 0.332 | Gen: ethay airway onloningcay isway orkingway\n","Epoch:  39 | Train loss: 0.157 | Val loss: 0.289 | Gen: ethay airway oningcay-ongcay isway orkingway\n","Epoch:  40 | Train loss: 0.153 | Val loss: 0.343 | Gen: ethay airway onditiondionway isway orkingway\n","Epoch:  41 | Train loss: 0.174 | Val loss: 0.341 | Gen: emay airway ongtionitingcay isway orkingway\n","Epoch:  42 | Train loss: 0.155 | Val loss: 0.324 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  43 | Train loss: 0.157 | Val loss: 0.347 | Gen: ethay airway ongcay-ongsay isway orkingway\n","Epoch:  44 | Train loss: 0.148 | Val loss: 0.326 | Gen: ethay airway onditionday isway orkingway\n","Epoch:  45 | Train loss: 0.124 | Val loss: 0.307 | Gen: ethay airway onditiondionway isway orkingway\n","Epoch:  46 | Train loss: 0.112 | Val loss: 0.297 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  47 | Train loss: 0.106 | Val loss: 0.292 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  48 | Train loss: 0.103 | Val loss: 0.290 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  49 | Train loss: 0.100 | Val loss: 0.291 | Gen: ethay airway onditionday isway orkingway\n","Epoch:  50 | Train loss: 0.098 | Val loss: 0.293 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  51 | Train loss: 0.096 | Val loss: 0.295 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  52 | Train loss: 0.094 | Val loss: 0.294 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  53 | Train loss: 0.092 | Val loss: 0.298 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  54 | Train loss: 0.093 | Val loss: 0.297 | Gen: ethay airway onditionday isway orkingway\n","Epoch:  55 | Train loss: 0.092 | Val loss: 0.315 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  56 | Train loss: 0.095 | Val loss: 0.313 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  57 | Train loss: 0.109 | Val loss: 0.307 | Gen: ethay airway ongcay-onitingcay isway orkingway\n","Epoch:  58 | Train loss: 0.099 | Val loss: 0.317 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  59 | Train loss: 0.129 | Val loss: 0.287 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  60 | Train loss: 0.142 | Val loss: 0.418 | Gen: emay airway onditioncay isway orkingway\n","Epoch:  61 | Train loss: 0.145 | Val loss: 0.332 | Gen: ethay airway onditioncay isway orkingway\n","Epoch:  62 | Train loss: 0.104 | Val loss: 0.317 | Gen: ethay airway onditiondionway isway orkingway\n","Epoch:  63 | Train loss: 0.101 | Val loss: 0.314 | Gen: ethay airway onditionitionway isway orkingway\n","Epoch:  64 | Train loss: 0.090 | Val loss: 0.317 | Gen: ethtthay airway ondititingcay isway orkingway\n","Epoch:  65 | Train loss: 0.092 | Val loss: 0.323 | Gen: ethay airway ondititititititititi isway orkingway\n","Epoch:  66 | Train loss: 0.087 | Val loss: 0.296 | Gen: ethay airway ondititingcay isway orkingway\n","Epoch:  67 | Train loss: 0.076 | Val loss: 0.295 | Gen: ethay airway ondititititingcay isway orkingway\n","Epoch:  68 | Train loss: 0.073 | Val loss: 0.296 | Gen: ethay airway ondititititititititi isway orkingway\n","Epoch:  69 | Train loss: 0.070 | Val loss: 0.295 | Gen: ethay airway ondititingcay isway orkingway\n","Epoch:  70 | Train loss: 0.069 | Val loss: 0.296 | Gen: ethay airway ondititititititingca isway orkingway\n","Epoch:  71 | Train loss: 0.068 | Val loss: 0.296 | Gen: ethay airway ondititingcay isway orkingway\n","Epoch:  72 | Train loss: 0.067 | Val loss: 0.298 | Gen: ethay airway ondititionday isway orkingway\n","Epoch:  73 | Train loss: 0.066 | Val loss: 0.303 | Gen: ethay airway ondititionday isway orkingway\n","Epoch:  74 | Train loss: 0.065 | Val loss: 0.304 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  75 | Train loss: 0.065 | Val loss: 0.312 | Gen: ethay airway ondititititingcay isway orkingway\n","Epoch:  76 | Train loss: 0.064 | Val loss: 0.309 | Gen: ethay airway onditiondititingcay isway orkingway\n","Epoch:  77 | Train loss: 0.063 | Val loss: 0.314 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  78 | Train loss: 0.063 | Val loss: 0.314 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  79 | Train loss: 0.061 | Val loss: 0.318 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  80 | Train loss: 0.063 | Val loss: 0.328 | Gen: ethay airway ondititionditgcay isway orkingway\n","Epoch:  81 | Train loss: 0.067 | Val loss: 0.348 | Gen: ethay airway onditiondioncay isway orkingway\n","Epoch:  82 | Train loss: 0.137 | Val loss: 0.455 | Gen: ethay airway onditingsay isway orkingway\n","Epoch:  83 | Train loss: 0.178 | Val loss: 0.333 | Gen: ethay airway onguingsay isway orkingway\n","Epoch:  84 | Train loss: 0.115 | Val loss: 0.318 | Gen: ethay airway ondititititititititi isway orkingway\n","Epoch:  85 | Train loss: 0.076 | Val loss: 0.276 | Gen: ethay airway onditionditionday isway orkingway\n","Epoch:  86 | Train loss: 0.064 | Val loss: 0.284 | Gen: ethay airway onditionditititititi isway orkingway\n","Epoch:  87 | Train loss: 0.059 | Val loss: 0.291 | Gen: ethay airway onditionditingcay isway orkingway\n","Epoch:  88 | Train loss: 0.057 | Val loss: 0.294 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  89 | Train loss: 0.055 | Val loss: 0.296 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  90 | Train loss: 0.053 | Val loss: 0.300 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  91 | Train loss: 0.052 | Val loss: 0.301 | Gen: ethay airway onditionditingcay isway orkingway\n","Epoch:  92 | Train loss: 0.051 | Val loss: 0.304 | Gen: ethay airway onditionditititingca isway orkingway\n","Epoch:  93 | Train loss: 0.051 | Val loss: 0.307 | Gen: ethay airway onditionditititingca isway orkingway\n","Epoch:  94 | Train loss: 0.050 | Val loss: 0.309 | Gen: ethay airway onditionditgcay isway orkingway\n","Epoch:  95 | Train loss: 0.049 | Val loss: 0.311 | Gen: ethay airway onditionditgcay isway orkingway\n","Epoch:  96 | Train loss: 0.048 | Val loss: 0.313 | Gen: ethay airway onditionditgcay isway orkingway\n","Epoch:  97 | Train loss: 0.048 | Val loss: 0.314 | Gen: ethay airway onditionditititingca isway orkingway\n","Epoch:  98 | Train loss: 0.048 | Val loss: 0.315 | Gen: ethay airway onditionditgcay isway orkingway\n","Epoch:  99 | Train loss: 0.047 | Val loss: 0.317 | Gen: ethay airway onditionditionday isway orkingway\n","source:\t\tthe air conditioning is working \n","translated:\tethay airway onditionditionday isway orkingway\n"],"name":"stdout"}]},{"metadata":{"id":"vE-hKCxhF3iR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"700fd4df-ccf4-4aff-89f1-c6e2b48a723d","executionInfo":{"status":"ok","timestamp":1552955185098,"user_tz":240,"elapsed":444107,"user":{"displayName":"Leo Ouyang","photoUrl":"","userId":"13328508759078244531"}}},"cell_type":"code","source":["TEST_SENTENCE = 'but you are encouraged to submit a proposal earlier'\n","translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["source:\t\tbut you are encouraged to submit a proposal earlier \n","translated:\tutbay youway areway encouragencouragedwa otay ubmitsay away oposporetpray erlierliearway\n"],"name":"stdout"}]},{"metadata":{"id":"NvIaYPzPb8bK","colab_type":"text"},"cell_type":"markdown","source":["## RNN scaled dot-product attention decoder"]},{"metadata":{"id":"D8iMxQc-cSIp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":2249},"outputId":"94c2b660-1114-478d-e9b3-96f8393d7986","executionInfo":{"status":"ok","timestamp":1552958145371,"user_tz":240,"elapsed":734691,"user":{"displayName":"Leo Ouyang","photoUrl":"","userId":"13328508759078244531"}}},"cell_type":"code","source":["TEST_SENTENCE = 'the air conditioning is working'\n","\n","torch.manual_seed(1)\n","args = AttrDict()\n","args_dict = {\n","              'cuda':True, \n","              'nepochs':100, \n","              'checkpoint_dir':\"checkpoints\", \n","              'learning_rate':0.005, \n","              'lr_decay':0.99,\n","              'batch_size':64, \n","              'hidden_size':20, \n","              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n","              'attention_type': 'scaled_dot',  # options: additive / scaled_dot\n","}\n","args.update(args_dict)\n","\n","print_opts(args)\n","rnn_attn_encoder_scaled_dot, rnn_attn_decoder_scaled_dot = train(args)\n","\n","translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder_scaled_dot, rnn_attn_decoder_scaled_dot, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":44,"outputs":[{"output_type":"stream","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                            hidden_size: 20                                     \n","                          learning_rate: 0.005                                  \n","                             batch_size: 64                                     \n","                                nepochs: 100                                    \n","                                   cuda: 1                                      \n","                         checkpoint_dir: checkpoints                            \n","                           decoder_type: rnn_attention                          \n","                               lr_decay: 0.99                                   \n","                         attention_type: scaled_dot                             \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('payment', 'aymentpay')\n","('ordination', 'ordinationway')\n","('amends', 'amendsway')\n","('principally', 'incipallypray')\n","('anybody', 'anybodyway')\n","Num unique word pairs: 6387\n","Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 2.334 | Val loss: 1.944 | Gen: ereray ay ontay intay ontay\n","Epoch:   1 | Train loss: 1.841 | Val loss: 1.692 | Gen: eray aray onsingingingingingin iiiiiiiiiiiiiiiiiiii onsinghay\n","Epoch:   2 | Train loss: 1.610 | Val loss: 1.509 | Gen: eray arisay onsiongay isisisisay onsingingay\n","Epoch:   3 | Train loss: 1.452 | Val loss: 1.394 | Gen: elay aristay onsingingay isisistay oningingay\n","Epoch:   4 | Train loss: 1.305 | Val loss: 1.286 | Gen: eltay aristiray onininininininininin issay oningsay\n","Epoch:   5 | Train loss: 1.190 | Val loss: 1.202 | Gen: etay aistay oniningray ististay orkingray\n","Epoch:   6 | Train loss: 1.086 | Val loss: 1.067 | Gen: etay airiray oniningray istay oningray\n","Epoch:   7 | Train loss: 0.978 | Val loss: 1.009 | Gen: etay airway oninininingday istay orkingway\n","Epoch:   8 | Train loss: 0.907 | Val loss: 0.917 | Gen: ethay airray onininingsay isgay orkingcay\n","Epoch:   9 | Train loss: 0.812 | Val loss: 0.839 | Gen: ethay airsay onsingsay issay orkingsay\n","Epoch:  10 | Train loss: 0.723 | Val loss: 0.741 | Gen: etlay airway onininingcay issay orkingway\n","Epoch:  11 | Train loss: 0.657 | Val loss: 0.713 | Gen: ethay airsay onininingsay isshay orkingway\n","Epoch:  12 | Train loss: 0.632 | Val loss: 0.713 | Gen: etay airsay ondinindindindindind issway orkingway\n","Epoch:  13 | Train loss: 0.570 | Val loss: 0.638 | Gen: ethay airway onininingcay issway orkingway\n","Epoch:  14 | Train loss: 0.535 | Val loss: 0.606 | Gen: etay airway onininingcay issway orkingway\n","Epoch:  15 | Train loss: 0.467 | Val loss: 0.546 | Gen: ethay airway onininingcay isway orkingway\n","Epoch:  16 | Train loss: 0.430 | Val loss: 0.535 | Gen: ethay airway onscay isway orkingway\n","Epoch:  17 | Train loss: 0.408 | Val loss: 0.522 | Gen: ethay airgway oniningcay isway orkingway\n","Epoch:  18 | Train loss: 0.408 | Val loss: 0.494 | Gen: ethay airway oningcay isway orkingway\n","Epoch:  19 | Train loss: 0.375 | Val loss: 0.485 | Gen: ethay airway oningcay isway orkingway\n","Epoch:  20 | Train loss: 0.345 | Val loss: 0.456 | Gen: ethay airway oningcay isway orkingway\n","Epoch:  21 | Train loss: 0.332 | Val loss: 0.446 | Gen: ethay airway oniningcay isway orkingway\n","Epoch:  22 | Train loss: 0.315 | Val loss: 0.444 | Gen: ethay airway onitingcay isway orkingway\n","Epoch:  23 | Train loss: 0.303 | Val loss: 0.439 | Gen: ethay airway ondingcay isway orkingway\n","Epoch:  24 | Train loss: 0.288 | Val loss: 0.445 | Gen: ethay airway onitingcay isway orkingway\n","Epoch:  25 | Train loss: 0.280 | Val loss: 0.425 | Gen: ethay airway onitingcay isway orkingway\n","Epoch:  26 | Train loss: 0.353 | Val loss: 0.543 | Gen: ethay airray onitingcay issay orkingway\n","Epoch:  27 | Train loss: 0.325 | Val loss: 0.477 | Gen: ethay airway ondingcay isway orkingway\n","Epoch:  28 | Train loss: 0.288 | Val loss: 0.415 | Gen: ethay airway ondingcay isway orkingway\n","Epoch:  29 | Train loss: 0.267 | Val loss: 0.450 | Gen: ethay airway onitiningcay isway orkingway\n","Epoch:  30 | Train loss: 0.246 | Val loss: 0.395 | Gen: ethay airway ondingcay isway orkingway\n","Epoch:  31 | Train loss: 0.216 | Val loss: 0.375 | Gen: ethay airway ondinitingcay isway orkingway\n","Epoch:  32 | Train loss: 0.205 | Val loss: 0.367 | Gen: ethay airway ondingcay isway orkingway\n","Epoch:  33 | Train loss: 0.198 | Val loss: 0.365 | Gen: ethay airway ondingcay isway orkingway\n","Epoch:  34 | Train loss: 0.189 | Val loss: 0.362 | Gen: ethay airway onscay isway orkingway\n","Epoch:  35 | Train loss: 0.183 | Val loss: 0.354 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  36 | Train loss: 0.176 | Val loss: 0.357 | Gen: ethay airway onscay isway orkingway\n","Epoch:  37 | Train loss: 0.181 | Val loss: 0.370 | Gen: ethay airway onscay isway orkingway\n","Epoch:  38 | Train loss: 0.195 | Val loss: 0.382 | Gen: ethay airway ondingcay isway orkingway\n","Epoch:  39 | Train loss: 0.223 | Val loss: 0.423 | Gen: ethay airway ondingcay isway orkingway\n","Epoch:  40 | Train loss: 0.248 | Val loss: 0.435 | Gen: ethay airway onitingcay isway orkingway\n","Epoch:  41 | Train loss: 0.253 | Val loss: 0.389 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  42 | Train loss: 0.208 | Val loss: 0.364 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  43 | Train loss: 0.173 | Val loss: 0.337 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  44 | Train loss: 0.158 | Val loss: 0.332 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  45 | Train loss: 0.150 | Val loss: 0.335 | Gen: ethay airway onditionscay isway orkingway\n","Epoch:  46 | Train loss: 0.146 | Val loss: 0.324 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  47 | Train loss: 0.142 | Val loss: 0.337 | Gen: ethay airway onditionscay isway orkingway\n","Epoch:  48 | Train loss: 0.144 | Val loss: 0.328 | Gen: ethay airway onditionscay isway orkingway\n","Epoch:  49 | Train loss: 0.146 | Val loss: 0.350 | Gen: ethay airway onscay isway orkingway\n","Epoch:  50 | Train loss: 0.154 | Val loss: 0.345 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  51 | Train loss: 0.143 | Val loss: 0.353 | Gen: ethay airway ondititionscay isway orkingway\n","Epoch:  52 | Train loss: 0.141 | Val loss: 0.333 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  53 | Train loss: 0.127 | Val loss: 0.329 | Gen: ethay airway onditionscay isway orkingway\n","Epoch:  54 | Train loss: 0.124 | Val loss: 0.330 | Gen: ethay airway onditionscay isway orkingway\n","Epoch:  55 | Train loss: 0.119 | Val loss: 0.340 | Gen: ethay airway onscay isway orkingway\n","Epoch:  56 | Train loss: 0.116 | Val loss: 0.328 | Gen: ethay airway onditionscay isway orkingway\n","Epoch:  57 | Train loss: 0.114 | Val loss: 0.341 | Gen: ethay airway onscay isway orkingway\n","Epoch:  58 | Train loss: 0.114 | Val loss: 0.344 | Gen: ethay airway onditionscay isway orkingway\n","Epoch:  59 | Train loss: 0.117 | Val loss: 0.355 | Gen: ethay airway onditionscay isway orkingway\n","Epoch:  60 | Train loss: 0.183 | Val loss: 0.432 | Gen: ethay airway onscay isway orkway\n","Epoch:  61 | Train loss: 0.238 | Val loss: 0.472 | Gen: ethay airway ondinitionpay isway orkingway\n","Epoch:  62 | Train loss: 0.196 | Val loss: 0.371 | Gen: ethay airway onitingcay isway orkingway\n","Epoch:  63 | Train loss: 0.149 | Val loss: 0.343 | Gen: ethay airway onditiondionday isway orkingway\n","Epoch:  64 | Train loss: 0.134 | Val loss: 0.341 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  65 | Train loss: 0.116 | Val loss: 0.317 | Gen: ethay airway onditionscay isway orkingway\n","Epoch:  66 | Train loss: 0.104 | Val loss: 0.309 | Gen: ethay airway onditiondionscay isway orkingway\n","Epoch:  67 | Train loss: 0.099 | Val loss: 0.311 | Gen: ethay airway ondiondionscay isway orkingway\n","Epoch:  68 | Train loss: 0.097 | Val loss: 0.310 | Gen: ethay airway onditiondionscay isway orkingway\n","Epoch:  69 | Train loss: 0.095 | Val loss: 0.313 | Gen: ethay airway ondionscay isway orkingway\n","Epoch:  70 | Train loss: 0.097 | Val loss: 0.316 | Gen: ethay airway onditionscay isway orkingway\n","Epoch:  71 | Train loss: 0.102 | Val loss: 0.323 | Gen: ethay airway ondionscay isway orkingway\n","Epoch:  72 | Train loss: 0.098 | Val loss: 0.311 | Gen: ethay airway ondiondionscay isway orkingway\n","Epoch:  73 | Train loss: 0.094 | Val loss: 0.314 | Gen: ethay airway ondionitionday isway orkingway\n","Epoch:  74 | Train loss: 0.090 | Val loss: 0.314 | Gen: ethay airway ondiondionscay isway orkingway\n","Epoch:  75 | Train loss: 0.089 | Val loss: 0.311 | Gen: ethay airway ondionitionscay isway orkingway\n","Epoch:  76 | Train loss: 0.087 | Val loss: 0.315 | Gen: ethay airway ondionitionscay isway orkingway\n","Epoch:  77 | Train loss: 0.086 | Val loss: 0.315 | Gen: ethay airway ondionitionscay isway orkingway\n","Epoch:  78 | Train loss: 0.088 | Val loss: 0.317 | Gen: ethay airway onditionscay isway orkingway\n","Epoch:  79 | Train loss: 0.115 | Val loss: 0.345 | Gen: ethay airway ondiondionscay isway orkingway\n","Epoch:  80 | Train loss: 0.188 | Val loss: 0.386 | Gen: ethay airway ondionitionay isway orkingway\n","Epoch:  81 | Train loss: 0.151 | Val loss: 0.336 | Gen: ethay airway ondiondionday isway orkingway\n","Epoch:  82 | Train loss: 0.113 | Val loss: 0.338 | Gen: ethay airway onitinitioncay isway orkingway\n","Epoch:  83 | Train loss: 0.098 | Val loss: 0.307 | Gen: ethay airway ondiondiondionscay isway orkingway\n","Epoch:  84 | Train loss: 0.086 | Val loss: 0.298 | Gen: ethay airway ondionitionscay isway orkingway\n","Epoch:  85 | Train loss: 0.081 | Val loss: 0.297 | Gen: ethay airway ondionitionscay isway orkingway\n","Epoch:  86 | Train loss: 0.079 | Val loss: 0.298 | Gen: ethay airway ondionitionscay isway orkingway\n","Epoch:  87 | Train loss: 0.077 | Val loss: 0.299 | Gen: ethay airway ondionitionscay isway orkingway\n","Epoch:  88 | Train loss: 0.076 | Val loss: 0.300 | Gen: ethay airway ondionitionscay isway orkingway\n","Epoch:  89 | Train loss: 0.075 | Val loss: 0.300 | Gen: ethay airway ondionitionscay isway orkingway\n","Epoch:  90 | Train loss: 0.074 | Val loss: 0.301 | Gen: ethay airway ondionitionscay isway orkingway\n","Epoch:  91 | Train loss: 0.073 | Val loss: 0.302 | Gen: ethay airway ondionitionscay isway orkingway\n","Epoch:  92 | Train loss: 0.073 | Val loss: 0.304 | Gen: ethay airway ondionitionscay isway orkingway\n","Epoch:  93 | Train loss: 0.072 | Val loss: 0.303 | Gen: ethay airway ondionitionscay isway orkingway\n","Epoch:  94 | Train loss: 0.072 | Val loss: 0.306 | Gen: ethay airway onditionitionscay isway orkingway\n","Epoch:  95 | Train loss: 0.071 | Val loss: 0.306 | Gen: ethay airway onditionitionscay isway orkingway\n","Epoch:  96 | Train loss: 0.070 | Val loss: 0.311 | Gen: ethay airway onditionitionscay isway orkingway\n","Epoch:  97 | Train loss: 0.069 | Val loss: 0.306 | Gen: ethay airway onditionitionscay isway orkingway\n","Epoch:  98 | Train loss: 0.070 | Val loss: 0.310 | Gen: ethay airway onditionitionscay isway orkingway\n","Epoch:  99 | Train loss: 0.071 | Val loss: 0.314 | Gen: ethay airway ondionitionscay isway orkingway\n","source:\t\tthe air conditioning is working \n","translated:\tethay airway ondionitionscay isway orkingway\n"],"name":"stdout"}]},{"metadata":{"id":"X8FaZZUWRpY9","colab_type":"text"},"cell_type":"markdown","source":["## Transformer"]},{"metadata":{"id":"Ik5rx9qw9KCg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":2249},"outputId":"b1a9bc24-c2b1-4619-fb52-1afa3648f5ec","executionInfo":{"status":"ok","timestamp":1552957323538,"user_tz":240,"elapsed":402771,"user":{"displayName":"Leo Ouyang","photoUrl":"","userId":"13328508759078244531"}}},"cell_type":"code","source":["TEST_SENTENCE = 'the air conditioning is working'\n","\n","torch.manual_seed(1)\n","args = AttrDict()\n","args_dict = {\n","              'cuda':True, \n","              'nepochs':100, \n","              'checkpoint_dir':\"checkpoints\", \n","              'learning_rate':0.005, \n","              'lr_decay':0.99,\n","              'batch_size':64, \n","              'hidden_size':20, \n","              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n","              'num_transformer_layers': 3,\n","}\n","args.update(args_dict)\n","\n","print_opts(args)\n","transformer_encoder, transformer_decoder = train(args)\n","\n","translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":40,"outputs":[{"output_type":"stream","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                            hidden_size: 20                                     \n","                          learning_rate: 0.005                                  \n","                 num_transformer_layers: 3                                      \n","                             batch_size: 64                                     \n","                                nepochs: 100                                    \n","                                   cuda: 1                                      \n","                         checkpoint_dir: checkpoints                            \n","                           decoder_type: transformer                            \n","                               lr_decay: 0.99                                   \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('payment', 'aymentpay')\n","('ordination', 'ordinationway')\n","('amends', 'amendsway')\n","('principally', 'incipallypray')\n","('anybody', 'anybodyway')\n","Num unique word pairs: 6387\n","Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 2.159 | Val loss: 1.689 | Gen: eay arisay ingdey isissississsississis ingray\n","Epoch:   1 | Train loss: 1.528 | Val loss: 1.318 | Gen: ettay airlay oningingingsiongings isisisisissisissisis oringingray\n","Epoch:   2 | Train loss: 1.172 | Val loss: 1.111 | Gen: ethay airsay ingsingingscay isway oringinghay\n","Epoch:   3 | Train loss: 0.969 | Val loss: 0.897 | Gen: ethay airay onitindingby isway orkingway\n","Epoch:   4 | Train loss: 0.814 | Val loss: 0.806 | Gen: eathay airway onitiongcay isway orkwaingway\n","Epoch:   5 | Train loss: 0.679 | Val loss: 0.707 | Gen: ethay airway onitiondionditiondca isway orkingway\n","Epoch:   6 | Train loss: 0.614 | Val loss: 0.597 | Gen: ethay airway onitiondcay isway orkingway\n","Epoch:   7 | Train loss: 0.526 | Val loss: 0.580 | Gen: ethay airway onitiongcay isway orkway\n","Epoch:   8 | Train loss: 0.448 | Val loss: 0.512 | Gen: ethay airway onitionbcay isway orkingway\n","Epoch:   9 | Train loss: 0.394 | Val loss: 0.531 | Gen: ethay airay onditiongcay isway orkingway\n","Epoch:  10 | Train loss: 0.324 | Val loss: 0.429 | Gen: ethay airway onitiongcay isway orkingway\n","Epoch:  11 | Train loss: 0.280 | Val loss: 0.394 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  12 | Train loss: 0.266 | Val loss: 0.402 | Gen: ethay airway onitiongcay isway orkingway\n","Epoch:  13 | Train loss: 0.270 | Val loss: 0.432 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  14 | Train loss: 0.240 | Val loss: 0.275 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  15 | Train loss: 0.181 | Val loss: 0.344 | Gen: ethewhay airway onditiongcay isway orkingway\n","Epoch:  16 | Train loss: 0.210 | Val loss: 0.341 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  17 | Train loss: 0.190 | Val loss: 0.307 | Gen: eethay airway ondioniniongcay isway ororkingway\n","Epoch:  18 | Train loss: 0.166 | Val loss: 0.363 | Gen: ethay airway onditingcay isway orkingingway\n","Epoch:  19 | Train loss: 0.173 | Val loss: 0.242 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  20 | Train loss: 0.128 | Val loss: 0.252 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  21 | Train loss: 0.111 | Val loss: 0.202 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  22 | Train loss: 0.127 | Val loss: 0.204 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  23 | Train loss: 0.120 | Val loss: 0.217 | Gen: ethay airway onditioniningcay isway orkingway\n","Epoch:  24 | Train loss: 0.149 | Val loss: 0.202 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  25 | Train loss: 0.138 | Val loss: 0.218 | Gen: ethay airway ondiiongcay isway orkingway\n","Epoch:  26 | Train loss: 0.128 | Val loss: 0.236 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  27 | Train loss: 0.078 | Val loss: 0.134 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  28 | Train loss: 0.073 | Val loss: 0.182 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  29 | Train loss: 0.081 | Val loss: 0.157 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  30 | Train loss: 0.112 | Val loss: 0.179 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  31 | Train loss: 0.068 | Val loss: 0.153 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  32 | Train loss: 0.042 | Val loss: 0.121 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  33 | Train loss: 0.034 | Val loss: 0.099 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  34 | Train loss: 0.051 | Val loss: 0.120 | Gen: ethay airway onditionicay isway orkingway\n","Epoch:  35 | Train loss: 0.084 | Val loss: 0.204 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  36 | Train loss: 0.074 | Val loss: 0.182 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  37 | Train loss: 0.052 | Val loss: 0.119 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  38 | Train loss: 0.058 | Val loss: 0.115 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  39 | Train loss: 0.078 | Val loss: 0.134 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  40 | Train loss: 0.094 | Val loss: 0.160 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  41 | Train loss: 0.101 | Val loss: 0.133 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  42 | Train loss: 0.057 | Val loss: 0.099 | Gen: ethay airway ondiwiongcay isway orkingway\n","Epoch:  43 | Train loss: 0.038 | Val loss: 0.078 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  44 | Train loss: 0.030 | Val loss: 0.071 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  45 | Train loss: 0.027 | Val loss: 0.072 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  46 | Train loss: 0.019 | Val loss: 0.048 | Gen: ethay airway onditioniongcay isway orkingway\n","Epoch:  47 | Train loss: 0.015 | Val loss: 0.057 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  48 | Train loss: 0.074 | Val loss: 0.182 | Gen: ethay airway ondititionbay isway orkingway\n","Epoch:  49 | Train loss: 0.141 | Val loss: 0.210 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  50 | Train loss: 0.105 | Val loss: 0.093 | Gen: ethay airway onditionioniongcay isway orkingway\n","Epoch:  51 | Train loss: 0.119 | Val loss: 0.158 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  52 | Train loss: 0.075 | Val loss: 0.139 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  53 | Train loss: 0.035 | Val loss: 0.051 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  54 | Train loss: 0.020 | Val loss: 0.058 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  55 | Train loss: 0.044 | Val loss: 0.049 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  56 | Train loss: 0.026 | Val loss: 0.060 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  57 | Train loss: 0.015 | Val loss: 0.059 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  58 | Train loss: 0.017 | Val loss: 0.085 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  59 | Train loss: 0.067 | Val loss: 0.097 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  60 | Train loss: 0.055 | Val loss: 0.099 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  61 | Train loss: 0.029 | Val loss: 0.074 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  62 | Train loss: 0.012 | Val loss: 0.048 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  63 | Train loss: 0.007 | Val loss: 0.047 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  64 | Train loss: 0.015 | Val loss: 0.047 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  65 | Train loss: 0.010 | Val loss: 0.046 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  66 | Train loss: 0.010 | Val loss: 0.047 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  67 | Train loss: 0.004 | Val loss: 0.042 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  68 | Train loss: 0.002 | Val loss: 0.044 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  69 | Train loss: 0.003 | Val loss: 0.052 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  70 | Train loss: 0.013 | Val loss: 0.106 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  71 | Train loss: 0.028 | Val loss: 0.092 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  72 | Train loss: 0.065 | Val loss: 0.195 | Gen: ethay airway onditioniciongcay isway orkingway\n","Epoch:  73 | Train loss: 0.156 | Val loss: 0.182 | Gen: ethay airway ondioniningcay isway orkingway\n","Epoch:  74 | Train loss: 0.128 | Val loss: 0.151 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  75 | Train loss: 0.070 | Val loss: 0.101 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  76 | Train loss: 0.038 | Val loss: 0.084 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  77 | Train loss: 0.018 | Val loss: 0.059 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  78 | Train loss: 0.008 | Val loss: 0.043 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  79 | Train loss: 0.003 | Val loss: 0.042 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  80 | Train loss: 0.002 | Val loss: 0.042 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  81 | Train loss: 0.002 | Val loss: 0.042 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  82 | Train loss: 0.002 | Val loss: 0.043 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  83 | Train loss: 0.001 | Val loss: 0.043 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  84 | Train loss: 0.001 | Val loss: 0.044 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  85 | Train loss: 0.001 | Val loss: 0.044 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  86 | Train loss: 0.001 | Val loss: 0.045 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  87 | Train loss: 0.001 | Val loss: 0.045 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  88 | Train loss: 0.001 | Val loss: 0.045 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  89 | Train loss: 0.001 | Val loss: 0.046 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  90 | Train loss: 0.001 | Val loss: 0.046 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  91 | Train loss: 0.001 | Val loss: 0.046 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  92 | Train loss: 0.000 | Val loss: 0.047 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  93 | Train loss: 0.000 | Val loss: 0.047 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  94 | Train loss: 0.000 | Val loss: 0.047 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  95 | Train loss: 0.000 | Val loss: 0.048 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  96 | Train loss: 0.000 | Val loss: 0.048 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  97 | Train loss: 0.000 | Val loss: 0.048 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  98 | Train loss: 0.000 | Val loss: 0.049 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  99 | Train loss: 0.000 | Val loss: 0.049 | Gen: ethay airway onditioningcay isway orkingway\n","source:\t\tthe air conditioning is working \n","translated:\tethay airway onditioningcay isway orkingway\n"],"name":"stdout"}]},{"metadata":{"id":"ULCMHm5ZF7vx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"c3b7f094-4658-4ed7-f15b-d6d6245b2e72","executionInfo":{"status":"ok","timestamp":1552958722328,"user_tz":240,"elapsed":691,"user":{"displayName":"Leo Ouyang","photoUrl":"","userId":"13328508759078244531"}}},"cell_type":"code","source":["TEST_SENTENCE = 'what do you observe when training this modified transformer'\n","translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":48,"outputs":[{"output_type":"stream","text":["source:\t\twhat do you observe when training this modified transformer \n","translated:\tatwhay oday youay observeway enwhay ainingtray isthay odifiedmay ansformertray\n"],"name":"stdout"}]},{"metadata":{"id":"f0YTly_AfzSC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":2249},"outputId":"9ee7814b-a6fa-4970-a429-4d1232590b87","executionInfo":{"status":"ok","timestamp":1552958625903,"user_tz":240,"elapsed":364772,"user":{"displayName":"Leo Ouyang","photoUrl":"","userId":"13328508759078244531"}}},"cell_type":"code","source":["TEST_SENTENCE = 'the air conditioning is working'\n","\n","torch.manual_seed(1)\n","args = AttrDict()\n","args_dict = {\n","              'cuda':True, \n","              'nepochs':100, \n","              'checkpoint_dir':\"checkpoints\", \n","              'learning_rate':0.005, \n","              'lr_decay':0.99,\n","              'batch_size':64, \n","              'hidden_size':20, \n","              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n","              'num_transformer_layers': 3,\n","}\n","args.update(args_dict)\n","\n","print_opts(args)\n","transformer_encoder_no_mask, transformer_decoder_no_mask = train(args)\n","\n","translated = translate_sentence(TEST_SENTENCE, transformer_encoder_no_mask, transformer_decoder_no_mask, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":47,"outputs":[{"output_type":"stream","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                            hidden_size: 20                                     \n","                          learning_rate: 0.005                                  \n","                 num_transformer_layers: 3                                      \n","                             batch_size: 64                                     \n","                                nepochs: 100                                    \n","                                   cuda: 1                                      \n","                         checkpoint_dir: checkpoints                            \n","                           decoder_type: transformer                            \n","                               lr_decay: 0.99                                   \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('payment', 'aymentpay')\n","('ordination', 'ordinationway')\n","('amends', 'amendsway')\n","('principally', 'incipallypray')\n","('anybody', 'anybodyway')\n","Num unique word pairs: 6387\n","Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 2.132 | Val loss: 1.634 | Gen:   ingcingcingcingcingc  \n","Epoch:   1 | Train loss: 1.515 | Val loss: 1.331 | Gen:   w w               \n","Epoch:   2 | Train loss: 1.237 | Val loss: 1.081 | Gen:                     \n","Epoch:   3 | Train loss: 1.040 | Val loss: 0.933 | Gen: ay aaaaaaaaaaaaaaaaaaaa   \n","Epoch:   4 | Train loss: 0.871 | Val loss: 0.821 | Gen:  airairairairairairai oninininiiininiiinnn  \n","Epoch:   5 | Train loss: 0.713 | Val loss: 0.700 | Gen:                     \n","Epoch:   6 | Train loss: 0.616 | Val loss: 0.652 | Gen:                     \n","Epoch:   7 | Train loss: 0.558 | Val loss: 0.593 | Gen: ethay   wwwwwwwwwwwwwwwwwwww \n","Epoch:   8 | Train loss: 0.527 | Val loss: 0.553 | Gen: ethay   wwwwwwwwwwwwwwwwwwww \n","Epoch:   9 | Train loss: 0.476 | Val loss: 0.564 | Gen: ethy  onininininininininin isisisisisisisisisis \n","Epoch:  10 | Train loss: 0.462 | Val loss: 0.514 | Gen: ethy  onininininininininin wwwwwwwwwwwwwwwwwwww \n","Epoch:  11 | Train loss: 0.447 | Val loss: 0.501 | Gen:   onicay wwwwwwwwwwwwwwwwwwww \n","Epoch:  12 | Train loss: 0.449 | Val loss: 0.511 | Gen: ethayEOSy  onwaywaay wwwwwwwwwwwwwwwwwwww orkwwwwwwwwwwwwwwwww\n","Epoch:  13 | Train loss: 0.421 | Val loss: 0.477 | Gen: ethy  onininininininininin  \n","Epoch:  14 | Train loss: 0.431 | Val loss: 0.496 | Gen:  airwairwairwairwairw ondiay  orkwwwwwwwwwwwwwwwww\n","Epoch:  15 | Train loss: 0.415 | Val loss: 0.473 | Gen:  airwairwairwairwairw onititititititititit isisisisisisisisisis orkingywy\n","Epoch:  16 | Train loss: 0.386 | Val loss: 0.443 | Gen:  awawawawawawawawawaw oniony isisisisisisisisisis orkwwwwwwwwwwwwwwwww\n","Epoch:  17 | Train loss: 0.401 | Val loss: 0.480 | Gen:   oniciy isisisisisisisisisis \n","Epoch:  18 | Train loss: 0.411 | Val loss: 0.471 | Gen:  airwairwairwairwairw onioniononononononon isisisisisisisisisis orkiny\n","Epoch:  19 | Train loss: 0.390 | Val loss: 0.459 | Gen:   onititititititititit isisisisisisisisisis orkingwaEOSEOSEOSy\n","Epoch:  20 | Train loss: 0.384 | Val loss: 0.437 | Gen:   onininininininininin isisisisisisisisisis orkingwaEOSy\n","Epoch:  21 | Train loss: 0.390 | Val loss: 0.461 | Gen:  airwairwairwairwairw onitntnttttttttttttt isisisisisisisisisis orkingwaEOSy\n","Epoch:  22 | Train loss: 0.379 | Val loss: 0.438 | Gen:   ondindindindindindin isay orkingywywy\n","Epoch:  23 | Train loss: 0.368 | Val loss: 0.426 | Gen:   ongcitistitititititi  \n","Epoch:  24 | Train loss: 0.373 | Val loss: 0.435 | Gen:   ondindindindindindin isisisisisisisisisis orkingingingingining\n","Epoch:  25 | Train loss: 0.384 | Val loss: 0.431 | Gen:   onionionionionionion isisisisisisisisisis orkingwaEOSy\n","Epoch:  26 | Train loss: 0.374 | Val loss: 0.416 | Gen:  airwairwairwairwairw ondindindindindindin isisisisisisisisisis orkingway\n","Epoch:  27 | Train loss: 0.357 | Val loss: 0.412 | Gen:   onititititititititit isisisisisisisisisis \n","Epoch:  28 | Train loss: 0.355 | Val loss: 0.409 | Gen: y airwairwairwairwairw ondititititititititi isisisisisisisisisis orkingwaEOSy\n","Epoch:  29 | Train loss: 0.354 | Val loss: 0.425 | Gen:   onininininininininin isisisisisisisisisis orkingay\n","Epoch:  30 | Train loss: 0.364 | Val loss: 0.417 | Gen: y  onitititititiitittit isisisisisisisisisis orkingway\n","Epoch:  31 | Train loss: 0.364 | Val loss: 0.435 | Gen:   onininininininininin isisisisisisisisisis orkingwaEOSy\n","Epoch:  32 | Train loss: 0.352 | Val loss: 0.403 | Gen:   onicay isisisisisisisisisis orkingway\n","Epoch:  33 | Train loss: 0.345 | Val loss: 0.407 | Gen:   ondititiitititititit isway orkingway\n","Epoch:  34 | Train loss: 0.344 | Val loss: 0.408 | Gen: ethay  onininininininininin isisisisisisisisisis orkigway\n","Epoch:  35 | Train loss: 0.342 | Val loss: 0.395 | Gen: ethay airairairairairairai ongcatEOSsatytstgtgtgt isisisisisisisisisis orkingay\n","Epoch:  36 | Train loss: 0.337 | Val loss: 0.398 | Gen: y airwairwairwairwairw onionionionionionion isway orkinway\n","Epoch:  37 | Train loss: 0.340 | Val loss: 0.407 | Gen: ethay  ondiondiondiondiondi isisisisisisisisisis orkingay\n","Epoch:  38 | Train loss: 0.343 | Val loss: 0.393 | Gen: y ay ondidy isisisisisisisisisis orkway\n","Epoch:  39 | Train loss: 0.345 | Val loss: 0.410 | Gen: y  ondititititititittit isisisisisisisisisis orkinway\n","Epoch:  40 | Train loss: 0.345 | Val loss: 0.395 | Gen: ethay airairairairairairai ondindindindindindin isisisisisisisisisis orkway\n","Epoch:  41 | Train loss: 0.338 | Val loss: 0.408 | Gen: y  onininininininininin isway orkinway\n","Epoch:  42 | Train loss: 0.340 | Val loss: 0.402 | Gen:   ondindindindindindin isisisisisisisisisis orkingwyEOSEOSEOSy\n","Epoch:  43 | Train loss: 0.335 | Val loss: 0.402 | Gen: ethay ay onininininininininin isway orkingay\n","Epoch:  44 | Train loss: 0.338 | Val loss: 0.399 | Gen: ethay airairairairairairai ondiondiondiondiondi isway orkingwaEOSEOSy\n","Epoch:  45 | Train loss: 0.336 | Val loss: 0.392 | Gen: ethay  onititititititititit isisisisisisisisisis orkingay\n","Epoch:  46 | Train loss: 0.327 | Val loss: 0.380 | Gen: y  ondiondiondiondiondi isway ovwkwkwkwkwkwkwkwkwk\n","Epoch:  47 | Train loss: 0.324 | Val loss: 0.387 | Gen: ethay airwairwairwairwairw onititititititititit isisisisisisisisisis orkingay\n","Epoch:  48 | Train loss: 0.326 | Val loss: 0.386 | Gen:   onionionionionionion isisisisisisisisisis orkinway\n","Epoch:  49 | Train loss: 0.324 | Val loss: 0.382 | Gen: ethay  onionionionionionion isayEOSy orkinway\n","Epoch:  50 | Train loss: 0.330 | Val loss: 0.385 | Gen: ethay  onininininininininin isisisisisisisisisis orkinway\n","Epoch:  51 | Train loss: 0.343 | Val loss: 0.390 | Gen:   ondiondiondiondiondi isway o\n","Epoch:  52 | Train loss: 0.331 | Val loss: 0.393 | Gen:                     \n","Epoch:  53 | Train loss: 0.328 | Val loss: 0.382 | Gen: y  ondititititititititi isayEOSy \n","Epoch:  54 | Train loss: 0.317 | Val loss: 0.390 | Gen:   onionionionionionion isway \n","Epoch:  55 | Train loss: 0.317 | Val loss: 0.382 | Gen:   ondititititittitiiti isisisisisisisisisis \n","Epoch:  56 | Train loss: 0.323 | Val loss: 0.391 | Gen:   onionionionionionion isisisisisisisisisis orkingay\n","Epoch:  57 | Train loss: 0.326 | Val loss: 0.384 | Gen:   onionionionionionion isisisisisisisisisis orkinway\n","Epoch:  58 | Train loss: 0.330 | Val loss: 0.384 | Gen:   onionionionionionion isway orkingay\n","Epoch:  59 | Train loss: 0.317 | Val loss: 0.377 | Gen:   onionionionionionion isasasasasasasasasas orkinway\n","Epoch:  60 | Train loss: 0.326 | Val loss: 0.376 | Gen:  ay onininininininininin isway orkingay\n","Epoch:  61 | Train loss: 0.318 | Val loss: 0.383 | Gen: y ay onininininininininin isisisisisisisisisis orkinway\n","Epoch:  62 | Train loss: 0.322 | Val loss: 0.395 | Gen:                     \n","Epoch:  63 | Train loss: 0.324 | Val loss: 0.387 | Gen:    isway            \n","Epoch:  64 | Train loss: 0.324 | Val loss: 0.370 | Gen:   ondititititititititi isisisisisisisisisis orkinway\n","Epoch:  65 | Train loss: 0.318 | Val loss: 0.367 | Gen: ethay  ondititititititititi isway orkinEOSay\n","Epoch:  66 | Train loss: 0.311 | Val loss: 0.365 | Gen:   oninininiininiininni  \n","Epoch:  67 | Train loss: 0.307 | Val loss: 0.361 | Gen: ethay  onionionionionionion isway \n","Epoch:  68 | Train loss: 0.305 | Val loss: 0.366 | Gen:   ondnnnnnninininninin  \n","Epoch:  69 | Train loss: 0.304 | Val loss: 0.366 | Gen: ethay  ondititititititititi isway orkingay\n","Epoch:  70 | Train loss: 0.303 | Val loss: 0.362 | Gen:   ondititititititititi isisisisisisisisisis ovy\n","Epoch:  71 | Train loss: 0.306 | Val loss: 0.366 | Gen: ethay  ondititititititititi isway orkingwy\n","Epoch:  72 | Train loss: 0.310 | Val loss: 0.365 | Gen:                     \n","Epoch:  73 | Train loss: 0.307 | Val loss: 0.373 | Gen:   onionionionionionion isway \n","Epoch:  74 | Train loss: 0.305 | Val loss: 0.366 | Gen:   onionionionionionion isisisisisisisisisis \n","Epoch:  75 | Train loss: 0.305 | Val loss: 0.366 | Gen:   onionionionionionion isway \n","Epoch:  76 | Train loss: 0.307 | Val loss: 0.369 | Gen:   ondiondiondiondiondo isway \n","Epoch:  77 | Train loss: 0.312 | Val loss: 0.374 | Gen:   ondiondiondiondiondi isisisisisisisisisis \n","Epoch:  78 | Train loss: 0.311 | Val loss: 0.374 | Gen: y  ondiondiondiondiondo isway orkinway\n","Epoch:  79 | Train loss: 0.306 | Val loss: 0.364 | Gen: y   isway           \n","Epoch:  80 | Train loss: 0.308 | Val loss: 0.362 | Gen:   onininininininininin isway \n","Epoch:  81 | Train loss: 0.303 | Val loss: 0.363 | Gen:   ondiondiondiondiondi isway \n","Epoch:  82 | Train loss: 0.298 | Val loss: 0.354 | Gen:   onininininininininin isway \n","Epoch:  83 | Train loss: 0.296 | Val loss: 0.353 | Gen: ethay  ondititititititititi isway orkinway\n","Epoch:  84 | Train loss: 0.307 | Val loss: 0.365 | Gen: y ay onionionionionionini isway orkinway\n","Epoch:  85 | Train loss: 0.301 | Val loss: 0.358 | Gen:   ondititititititititi  \n","Epoch:  86 | Train loss: 0.300 | Val loss: 0.359 | Gen:   onionionionionionion  \n","Epoch:  87 | Train loss: 0.298 | Val loss: 0.353 | Gen:   ondititititititittit isway \n","Epoch:  88 | Train loss: 0.296 | Val loss: 0.355 | Gen:  ay ondititititititititi isway orkinway\n","Epoch:  89 | Train loss: 0.300 | Val loss: 0.357 | Gen: y  ondindindindindindin isway orkinway\n","Epoch:  90 | Train loss: 0.302 | Val loss: 0.356 | Gen:   ondititititititititi isway orkinway\n","Epoch:  91 | Train loss: 0.296 | Val loss: 0.352 | Gen: y  onditititititittitti isway orkinway\n","Epoch:  92 | Train loss: 0.295 | Val loss: 0.357 | Gen:   ondinininininininiin isway orkingay\n","Epoch:  93 | Train loss: 0.296 | Val loss: 0.353 | Gen: y  ondiondiondiondiondi isway orkinway\n","Epoch:  94 | Train loss: 0.296 | Val loss: 0.355 | Gen:   ondititititititititi  orkingay\n","Epoch:  95 | Train loss: 0.297 | Val loss: 0.363 | Gen: y  onionionioniiononoon isway \n","Epoch:  96 | Train loss: 0.300 | Val loss: 0.355 | Gen:   onditititititititiit  \n","Epoch:  97 | Train loss: 0.298 | Val loss: 0.354 | Gen: y  onionionionioniionio isway \n","Epoch:  98 | Train loss: 0.292 | Val loss: 0.351 | Gen:    isway            \n","Epoch:  99 | Train loss: 0.292 | Val loss: 0.350 | Gen:   onionionionionionion isway \n","source:\t\tthe air conditioning is working \n","translated:\t  onionionionionionion isway \n"],"name":"stdout"}]},{"metadata":{"id":"qbfZCByITOI6","colab_type":"text"},"cell_type":"markdown","source":["# Attention visualization"]},{"metadata":{"id":"itCGMv3FdXsn","colab_type":"code","colab":{}},"cell_type":"code","source":["TEST_WORD_ATTN = 'racecar'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xBv4QQuBiU-V","colab_type":"text"},"cell_type":"markdown","source":["## Visualize RNN attention map"]},{"metadata":{"id":"aXvqoQYONMTA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":418},"outputId":"20d06365-ee9e-44af-c5ba-1bd76fff32d5","executionInfo":{"status":"ok","timestamp":1552967928386,"user_tz":240,"elapsed":852,"user":{"displayName":"Leo Ouyang","photoUrl":"","userId":"13328508759078244531"}}},"cell_type":"code","source":["visualize_attention(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, args)"],"execution_count":75,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAgwAAAGACAYAAADBHDoxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCCoAAAAASUVORK5CYII=\n","text/plain":["<Figure size 576x396 with 2 Axes>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["'acecarfhray'"]},"metadata":{"tags":[]},"execution_count":75}]},{"metadata":{"id":"xuOvxfA1NMz3","colab_type":"text"},"cell_type":"markdown","source":["## Visualize transformer attention maps from all the transformer layers"]},{"metadata":{"id":"HSSB4wd8-M7g","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1192},"outputId":"24d2f6a1-d5de-454b-a017-70c3f04757ea","executionInfo":{"status":"ok","timestamp":1552967929484,"user_tz":240,"elapsed":1934,"user":{"displayName":"Leo Ouyang","photoUrl":"","userId":"13328508759078244531"}}},"cell_type":"code","source":["visualize_attention(TEST_WORD_ATTN, transformer_encoder, transformer_decoder, None, args, )"],"execution_count":76,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAgsAAAGCCAYAAABuCIBDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 576x396 with 2 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAgsAAAGCCAYAAABuCIBDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiwAAABHhAUAAOCIsAAAABz9P8/nxjn4ig2GAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 576x396 with 2 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAgsAAAGCCAYAAABuCIBDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 576x396 with 2 Axes>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["'acecarray'"]},"metadata":{"tags":[]},"execution_count":76}]}]}